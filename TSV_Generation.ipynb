{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TSV_Generation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "182d05b10c5247dcb90f1f80e25958d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e40ac889416745ab9bab3ca7e8b3ee0d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_5130ac8f8b1e46cc90fe509c1cdc3450",
              "IPY_MODEL_b3bfc30affbd44d2a88a781486da195f"
            ]
          }
        },
        "e40ac889416745ab9bab3ca7e8b3ee0d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5130ac8f8b1e46cc90fe509c1cdc3450": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_7385557a178e4eefbeeba50ff6e768f1",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ce7fec646b7245da8df3ae0ca65d70af"
          }
        },
        "b3bfc30affbd44d2a88a781486da195f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d2ef31335bb24cff823b1f25e4ce331d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 251kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bff4ecdc16b74bd7aac85a3fd062d821"
          }
        },
        "7385557a178e4eefbeeba50ff6e768f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ce7fec646b7245da8df3ae0ca65d70af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d2ef31335bb24cff823b1f25e4ce331d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bff4ecdc16b74bd7aac85a3fd062d821": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "19724eded2ba4634b18121b0d3bd438e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7880f840603a4743bcdc716107646c49",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_598559f3b5b34ff9b86161cf107f069e",
              "IPY_MODEL_854781d1312241d69bc7748c90d1e928"
            ]
          }
        },
        "7880f840603a4743bcdc716107646c49": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "598559f3b5b34ff9b86161cf107f069e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_223ed6946b00498592f05feb841b08cd",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 433,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 433,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_69506ed91df842deac0b7fc80c8a9d61"
          }
        },
        "854781d1312241d69bc7748c90d1e928": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_cb3f947f000846eeb1a3960cb4419441",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 433/433 [00:07&lt;00:00, 56.4B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_67f501ade1af4b2f8738752fbb9eeb6a"
          }
        },
        "223ed6946b00498592f05feb841b08cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "69506ed91df842deac0b7fc80c8a9d61": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cb3f947f000846eeb1a3960cb4419441": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "67f501ade1af4b2f8738752fbb9eeb6a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "53355b2d6a2d4dbe95811728e7a060d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_691d976732514df0aaba310f6174ef7c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_dfed7028e9e142368ccd42b5c3e566da",
              "IPY_MODEL_9ddaf6454d9641e6be3855ddb746e2b3"
            ]
          }
        },
        "691d976732514df0aaba310f6174ef7c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dfed7028e9e142368ccd42b5c3e566da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_84ae707bed1f4d16827a2aaa583473f7",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 440473133,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 440473133,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cc02570915bb45f28c1784d96ec71b79"
          }
        },
        "9ddaf6454d9641e6be3855ddb746e2b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_bde5b15b491a4b7cab5c3ec299be2339",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 440M/440M [00:06&lt;00:00, 66.6MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_150c943c067f4d5a85f0ebb5f306c807"
          }
        },
        "84ae707bed1f4d16827a2aaa583473f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cc02570915bb45f28c1784d96ec71b79": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bde5b15b491a4b7cab5c3ec299be2339": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "150c943c067f4d5a85f0ebb5f306c807": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIrdzSGcAgJB",
        "outputId": "5d760dfc-ba34-480d-cb49-3285c1df0951"
      },
      "source": [
        "!pip install transformers==3.5.0\r\n",
        "!pip install nltk\r\n",
        "!pip install biobert-pytorch\r\n",
        "!pip install wget\r\n",
        "!pip install cookiecutter\r\n",
        "!pip install sentence-splitter"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers==3.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/34/fb092588df61bf33f113ade030d1cbe74fb73a0353648f8dd938a223dce7/transformers-3.5.0-py3-none-any.whl (1.3MB)\n",
            "\r\u001b[K     |▎                               | 10kB 20.7MB/s eta 0:00:01\r\u001b[K     |▌                               | 20kB 27.8MB/s eta 0:00:01\r\u001b[K     |▊                               | 30kB 24.5MB/s eta 0:00:01\r\u001b[K     |█                               | 40kB 19.2MB/s eta 0:00:01\r\u001b[K     |█▎                              | 51kB 14.4MB/s eta 0:00:01\r\u001b[K     |█▌                              | 61kB 14.1MB/s eta 0:00:01\r\u001b[K     |█▊                              | 71kB 15.0MB/s eta 0:00:01\r\u001b[K     |██                              | 81kB 14.5MB/s eta 0:00:01\r\u001b[K     |██▎                             | 92kB 14.8MB/s eta 0:00:01\r\u001b[K     |██▌                             | 102kB 13.0MB/s eta 0:00:01\r\u001b[K     |██▊                             | 112kB 13.0MB/s eta 0:00:01\r\u001b[K     |███                             | 122kB 13.0MB/s eta 0:00:01\r\u001b[K     |███▎                            | 133kB 13.0MB/s eta 0:00:01\r\u001b[K     |███▌                            | 143kB 13.0MB/s eta 0:00:01\r\u001b[K     |███▉                            | 153kB 13.0MB/s eta 0:00:01\r\u001b[K     |████                            | 163kB 13.0MB/s eta 0:00:01\r\u001b[K     |████▎                           | 174kB 13.0MB/s eta 0:00:01\r\u001b[K     |████▌                           | 184kB 13.0MB/s eta 0:00:01\r\u001b[K     |████▉                           | 194kB 13.0MB/s eta 0:00:01\r\u001b[K     |█████                           | 204kB 13.0MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 215kB 13.0MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 225kB 13.0MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 235kB 13.0MB/s eta 0:00:01\r\u001b[K     |██████                          | 245kB 13.0MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 256kB 13.0MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 266kB 13.0MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 276kB 13.0MB/s eta 0:00:01\r\u001b[K     |███████                         | 286kB 13.0MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 296kB 13.0MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 307kB 13.0MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 317kB 13.0MB/s eta 0:00:01\r\u001b[K     |████████                        | 327kB 13.0MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 337kB 13.0MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 348kB 13.0MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 358kB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████                       | 368kB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 378kB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 389kB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 399kB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████                      | 409kB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 419kB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 430kB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 440kB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████                     | 450kB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 460kB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 471kB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 481kB 13.0MB/s eta 0:00:01\r\u001b[K     |████████████                    | 491kB 13.0MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 501kB 13.0MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 512kB 13.0MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 522kB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 532kB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 542kB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 552kB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 563kB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 573kB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 583kB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 593kB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 604kB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 614kB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 624kB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 634kB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 645kB 13.0MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 655kB 13.0MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 665kB 13.0MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 675kB 13.0MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 686kB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 696kB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 706kB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 716kB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 727kB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 737kB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 747kB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 757kB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 768kB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 778kB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 788kB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 798kB 13.0MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 808kB 13.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 819kB 13.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 829kB 13.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 839kB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 849kB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 860kB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 870kB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 880kB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 890kB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 901kB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 911kB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 921kB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 931kB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 942kB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 952kB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 962kB 13.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 972kB 13.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 983kB 13.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 993kB 13.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.0MB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.0MB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.0MB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.0MB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.0MB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.1MB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.1MB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.1MB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.1MB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.1MB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 1.1MB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.1MB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.1MB 13.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.1MB 13.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.1MB 13.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 1.2MB 13.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.2MB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.2MB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.2MB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.2MB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.2MB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.2MB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.2MB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.2MB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.2MB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.3MB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.3MB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.3MB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.3MB 13.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.3MB 13.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.3MB 13.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (0.8)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (20.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (3.0.12)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (3.12.4)\n",
            "Collecting tokenizers==0.9.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/34/b39eb9994bc3c999270b69c9eea40ecc6f0e97991dba28282b9fd32d44ee/tokenizers-0.9.3-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 52.0MB/s \n",
            "\u001b[?25hCollecting sentencepiece==0.1.91\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 43.1MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 41.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (1.18.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3.5.0) (2.4.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers==3.5.0) (50.3.2)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers==3.5.0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.5.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.5.0) (0.17.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.5.0) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.5.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.5.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.5.0) (1.24.3)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=822a571b766a795575e8db32d5ebb24ab2f1e0c8fe2c86b59f84aa2c1837c8b5\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.9.3 transformers-3.5.0\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.15.0)\n",
            "Collecting biobert-pytorch\n",
            "  Downloading https://files.pythonhosted.org/packages/11/5a/d277c8f18208c8a605c7d33b960b7573f2cb4df0062c041ab77de9a8fc29/biobert_pytorch-0.9-py3-none-any.whl\n",
            "Installing collected packages: biobert-pytorch\n",
            "Successfully installed biobert-pytorch-0.9\n",
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9681 sha256=9b86cad0cc826e275f92a6700d05e6a8279fbde0324b0b267141c52c52adc13b\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n",
            "Collecting cookiecutter\n",
            "  Downloading https://files.pythonhosted.org/packages/95/83/83ebf950ec99b02c61719ccb116462844ba2e873df7c4d40afc962494312/cookiecutter-1.7.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: MarkupSafe<2.0.0 in /usr/local/lib/python3.6/dist-packages (from cookiecutter) (1.1.1)\n",
            "Requirement already satisfied: Jinja2<3.0.0 in /usr/local/lib/python3.6/dist-packages (from cookiecutter) (2.11.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.6/dist-packages (from cookiecutter) (7.1.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.6/dist-packages (from cookiecutter) (2.23.0)\n",
            "Collecting poyo>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/42/50/0b0820601bde2eda403f47b9a4a1f270098ed0dd4c00c443d883164bdccc/poyo-0.5.0-py2.py3-none-any.whl\n",
            "Collecting binaryornot>=0.4.4\n",
            "  Downloading https://files.pythonhosted.org/packages/24/7e/f7b6f453e6481d1e233540262ccbfcf89adcd43606f44a028d7f5fae5eb2/binaryornot-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from cookiecutter) (1.15.0)\n",
            "Requirement already satisfied: python-slugify>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from cookiecutter) (4.0.1)\n",
            "Collecting jinja2-time>=0.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/6a/a1/d44fa38306ffa34a7e1af09632b158e13ec89670ce491f8a15af3ebcb4e4/jinja2_time-0.2.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.23.0->cookiecutter) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.23.0->cookiecutter) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.23.0->cookiecutter) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.23.0->cookiecutter) (2020.12.5)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify>=4.0.0->cookiecutter) (1.3)\n",
            "Collecting arrow\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/bc/ebc1afb3c54377e128a01024c006f983d03ee124bc52392b78ba98c421b8/arrow-0.17.0-py2.py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.0 in /usr/local/lib/python3.6/dist-packages (from arrow->jinja2-time>=0.2.0->cookiecutter) (2.8.1)\n",
            "Installing collected packages: poyo, binaryornot, arrow, jinja2-time, cookiecutter\n",
            "Successfully installed arrow-0.17.0 binaryornot-0.4.4 cookiecutter-1.7.2 jinja2-time-0.2.0 poyo-0.5.0\n",
            "Collecting sentence-splitter\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4a/ae/3bd609c760d57849d7ddf223762f1881f3c4df6467f4eadb3a33652b7e0d/sentence_splitter-1.4-py2.py3-none-any.whl (44kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex>=2017.12.12 in /usr/local/lib/python3.6/dist-packages (from sentence-splitter) (2019.12.20)\n",
            "Installing collected packages: sentence-splitter\n",
            "Successfully installed sentence-splitter-1.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4JNdJmaAoP4",
        "outputId": "4bc14ed7-3904-4fc1-ea66-5461f2c31abd"
      },
      "source": [
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1R84voFKHfWV9xjzeLzWBbmY1uOMYpnyD' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1R84voFKHfWV9xjzeLzWBbmY1uOMYpnyD\" -O biobert_weights && rm -rf /tmp/cookies.txt\r\n",
        "\r\n",
        "!tar -xzf biobert_weights\r\n",
        "\r\n",
        "!transformers-cli convert --model_type bert --tf_checkpoint biobert_v1.1_pubmed/model.ckpt-1000000 --config biobert_v1.1_pubmed/bert_config.json --pytorch_dump_output biobert_v1.1_pubmed/pytorch_model.bin\r\n",
        "\r\n",
        "!mv biobert_v1.1_pubmed/bert_config.json biobert_v1.1_pubmed/config.json"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-12-16 06:01:30--  https://docs.google.com/uc?export=download&confirm=CpN9&id=1R84voFKHfWV9xjzeLzWBbmY1uOMYpnyD\n",
            "Resolving docs.google.com (docs.google.com)... 74.125.200.139, 74.125.200.101, 74.125.200.102, ...\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.200.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0g-c4-docs.googleusercontent.com/docs/securesc/8jps7uphnlvkas5vp9oo9mr1gue3q002/5358aeilrkgmfnlpl595fns41ik29ghj/1608098475000/13799006341648886493/10271237583061911360Z/1R84voFKHfWV9xjzeLzWBbmY1uOMYpnyD?e=download [following]\n",
            "--2020-12-16 06:01:30--  https://doc-0g-c4-docs.googleusercontent.com/docs/securesc/8jps7uphnlvkas5vp9oo9mr1gue3q002/5358aeilrkgmfnlpl595fns41ik29ghj/1608098475000/13799006341648886493/10271237583061911360Z/1R84voFKHfWV9xjzeLzWBbmY1uOMYpnyD?e=download\n",
            "Resolving doc-0g-c4-docs.googleusercontent.com (doc-0g-c4-docs.googleusercontent.com)... 74.125.68.132, 2404:6800:4003:c02::84\n",
            "Connecting to doc-0g-c4-docs.googleusercontent.com (doc-0g-c4-docs.googleusercontent.com)|74.125.68.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://docs.google.com/nonceSigner?nonce=0qeh12puqk386&continue=https://doc-0g-c4-docs.googleusercontent.com/docs/securesc/8jps7uphnlvkas5vp9oo9mr1gue3q002/5358aeilrkgmfnlpl595fns41ik29ghj/1608098475000/13799006341648886493/10271237583061911360Z/1R84voFKHfWV9xjzeLzWBbmY1uOMYpnyD?e%3Ddownload&hash=j2pt95kql61hp7slmriakjua6todnebe [following]\n",
            "--2020-12-16 06:01:31--  https://docs.google.com/nonceSigner?nonce=0qeh12puqk386&continue=https://doc-0g-c4-docs.googleusercontent.com/docs/securesc/8jps7uphnlvkas5vp9oo9mr1gue3q002/5358aeilrkgmfnlpl595fns41ik29ghj/1608098475000/13799006341648886493/10271237583061911360Z/1R84voFKHfWV9xjzeLzWBbmY1uOMYpnyD?e%3Ddownload&hash=j2pt95kql61hp7slmriakjua6todnebe\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.200.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://doc-0g-c4-docs.googleusercontent.com/docs/securesc/8jps7uphnlvkas5vp9oo9mr1gue3q002/5358aeilrkgmfnlpl595fns41ik29ghj/1608098475000/13799006341648886493/10271237583061911360Z/1R84voFKHfWV9xjzeLzWBbmY1uOMYpnyD?e=download&nonce=0qeh12puqk386&user=10271237583061911360Z&hash=4p5b8ljceodki29bvka8aite4ndb49ca [following]\n",
            "--2020-12-16 06:01:32--  https://doc-0g-c4-docs.googleusercontent.com/docs/securesc/8jps7uphnlvkas5vp9oo9mr1gue3q002/5358aeilrkgmfnlpl595fns41ik29ghj/1608098475000/13799006341648886493/10271237583061911360Z/1R84voFKHfWV9xjzeLzWBbmY1uOMYpnyD?e=download&nonce=0qeh12puqk386&user=10271237583061911360Z&hash=4p5b8ljceodki29bvka8aite4ndb49ca\n",
            "Connecting to doc-0g-c4-docs.googleusercontent.com (doc-0g-c4-docs.googleusercontent.com)|74.125.68.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/x-gzip]\n",
            "Saving to: ‘biobert_weights’\n",
            "\n",
            "biobert_weights         [         <=>        ] 382.81M   196MB/s    in 2.0s    \n",
            "\n",
            "2020-12-16 06:01:34 (196 MB/s) - ‘biobert_weights’ saved [401403346]\n",
            "\n",
            "2020-12-16 06:01:41.350116: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "Building PyTorch model from configuration: BertConfig {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "Converting TensorFlow checkpoint from /content/biobert_v1.1_pubmed/model.ckpt-1000000\n",
            "Loading TF weight bert/embeddings/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/embeddings/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/embeddings/position_embeddings with shape [512, 768]\n",
            "Loading TF weight bert/embeddings/token_type_embeddings with shape [2, 768]\n",
            "Loading TF weight bert/embeddings/word_embeddings with shape [28996, 768]\n",
            "Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_0/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_0/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_0/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_0/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_0/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_0/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_0/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_0/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_0/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_0/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_0/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_0/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_0/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_0/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_1/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_1/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_1/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_1/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_1/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_1/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_1/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_1/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_1/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_1/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_1/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_1/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_1/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_1/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_10/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_10/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_10/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_10/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_10/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_10/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_10/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_10/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_10/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_10/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_10/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_10/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_10/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_10/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_10/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_10/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_11/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_11/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_11/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_11/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_11/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_11/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_11/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_11/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_11/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_11/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_11/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_11/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_11/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_11/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_11/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_11/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_2/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_2/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_2/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_2/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_2/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_2/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_2/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_2/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_2/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_2/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_2/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_2/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_2/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_2/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_3/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_3/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_3/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_3/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_3/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_3/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_3/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_3/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_3/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_3/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_3/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_3/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_3/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_3/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_4/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_4/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_4/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_4/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_4/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_4/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_4/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_4/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_4/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_4/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_4/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_4/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_4/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_4/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_4/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_4/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_5/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_5/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_5/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_5/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_5/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_5/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_5/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_5/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_5/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_5/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_5/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_5/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_5/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_5/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_5/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_5/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_6/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_6/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_6/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_6/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_6/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_6/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_6/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_6/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_6/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_6/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_6/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_6/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_6/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_6/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_6/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_6/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_7/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_7/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_7/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_7/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_7/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_7/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_7/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_7/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_7/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_7/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_7/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_7/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_7/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_7/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_7/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_7/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_8/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_8/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_8/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_8/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_8/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_8/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_8/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_8/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_8/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_8/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_8/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_8/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_8/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_8/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_8/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_8/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_9/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_9/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_9/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_9/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_9/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_9/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_9/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_9/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_9/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_9/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_9/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_9/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_9/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_9/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_9/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_9/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/pooler/dense/bias with shape [768]\n",
            "Loading TF weight bert/pooler/dense/kernel with shape [768, 768]\n",
            "Initialize PyTorch weight ['bert', 'embeddings', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'embeddings', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'embeddings', 'position_embeddings']\n",
            "Initialize PyTorch weight ['bert', 'embeddings', 'token_type_embeddings']\n",
            "Initialize PyTorch weight ['bert', 'embeddings', 'word_embeddings']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'pooler', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'pooler', 'dense', 'kernel']\n",
            "Save PyTorch model to biobert_v1.1_pubmed/pytorch_model.bin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kfoQwc3AwVo",
        "outputId": "a3f0a4cf-b73c-4fe1-92ed-1b99d363dab5"
      },
      "source": [
        "import os\r\n",
        "import io\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import torch\r\n",
        "from transformers import BertTokenizer, BertModel, BertForPreTraining\r\n",
        "import nltk\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "import re\r\n",
        "from transformers import BertTokenizer, BertModel\r\n",
        "from transformers import BertForTokenClassification, AdamW\r\n",
        "from transformers import tokenization_utils_base\r\n",
        "import torch\r\n",
        "import torch.nn.functional as F\r\n",
        "import torch.nn as nn\r\n",
        "import pickle\r\n",
        "from torch.utils.data import TensorDataset, DataLoader\r\n",
        "from sentence_splitter import SentenceSplitter\r\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8RuawfTA2Hn"
      },
      "source": [
        "TEXT_PATH=\"/content/drive/My Drive/MeasEval-main/data/train/test/text\"\r\n",
        "TSV_PATH=\"/content/drive/My Drive/Dataset1/tsv\"\r\n",
        "MEASURED_ENTITY_MODEL_PATH=\"/content/drive/My Drive/Model/biobert_measured_entity.pt\"\r\n",
        "MEASURED_QUANTITY_MODEL_PATH=\"/content/drive/My Drive/Model/quantity_mixedloss_t55_sh.pt\"\r\n",
        "MEASURED_PROPERTY_MODEL_PATH=\"/content/drive/My Drive/Model/biobert_measured_prop.pt\"\r\n",
        "QUALIFIER_PATH=\"/content/drive/My Drive/Model/qualifier_20f1_t995.pt\"\r\n",
        "COUNT_MEASUREMENT_MODEL_PATH=\"/content/drive/My Drive/Model/count_t95_f70_ep20_sirdata.pt\"\r\n",
        "INFORMATION_DETECTION_PATH=\"/content/drive/My Drive/Model/info_60f1_t325.pt\"\r\n",
        "UNIT_DETECTION_PATH=\"/content/drive/My Drive/Model/units_t75_96f1.pt\"\r\n",
        "VOCAB_PATH=\"/content/drive/My Drive/Model/vocab.Field\""
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--YBsEMlBndI"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "model_BioBert = BertModel.from_pretrained('/content/biobert_v1.1_pubmed')\r\n",
        "tokenizer_BioBert = BertTokenizer(vocab_file='biobert_v1.1_pubmed/vocab.txt', do_lower_case=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jl2jnRNfA4ne",
        "outputId": "303c3800-f39c-49f0-dfd6-dd24114dbef1"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIanfdEtBQw8"
      },
      "source": [
        "files=os.listdir(TEXT_PATH)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X60ZcTVHA6yu"
      },
      "source": [
        "test_text=[]\r\n",
        "id=[]\r\n",
        "j=1\r\n",
        "for i in files:\r\n",
        "\r\n",
        "  pathtxt=os.path.join(TEXT_PATH,i)\r\n",
        "  \r\n",
        "  with open(pathtxt, 'r') as f:\r\n",
        "    text_in=f.read()\r\n",
        "  \r\n",
        "  splitter = SentenceSplitter(language='en')\r\n",
        "  test_text.append(splitter.split(text=text_in))\r\n",
        "  id.append(i[:-4])\r\n",
        "  j+=1"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhukswVKBNF-",
        "outputId": "03f62974-1eb8-4549-821c-5a73e56104dc"
      },
      "source": [
        "test_text"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['(F) Adult hRPE cultures display expression of markers typical of native RPE in their polarized localization.',\n",
              "  'DAPI is cyan, whereas all other immunofluorescence is gold.',\n",
              "  'Claudin 19, ezrin, ZO1, and MCT1 are preferentially located on the apical side.',\n",
              "  'RPE65 and CRALBP are cytoplasmic.',\n",
              "  'The scale bars represent 10 μm.'],\n",
              " ['GFP-expressing H9, BBHX8, and A1ATD-1 cells were generated by stable transfection using lipofectamine 2000 (Invitrogen) as described previously (Vallier et al., 2001).',\n",
              "  'GFP-positive cells were differentiated into foregut cells and then dissociated into single cells.',\n",
              "  'An individually isolated GFP cell was then transferred into a well containing non-GFP-positive hFSCs.',\n",
              "  'Wells were visually inspected 12 hr after plating, and wells containing a single GFP-positive hFSC were selected for clonal expansion.'],\n",
              " ['For group analyses, the following procedures were applied.',\n",
              "  \"(1) T1-weighted sMRIs from each subject (Fig. 1A) were registered to MNI space (Montreal Neurological Institute, MNI-152 atlas as in Fig. 1B) using an affine transformation (FLIRT–FMRIB's Linear Image Registration Tool) (Jenkinson and Smith, 2001) in FSL (www.fmrib.ox.ac.uk/fsl/).\",\n",
              "  \"(2) The cortical (Fig. 1C) and subcortical masks with pre-defined brain regions from the standard atlas were transferred to the individual's headspace (Fig. 1D), using the inverse of the transformation obtained in the first step: the Harvard-Oxford Atlas, part of the FSL software with masks of 96 cortical gray-matter regions (48 regions in each hemisphere), 21 sub-cortical regions, and cerebellum, was used.\",\n",
              "  '(3) The regional masks were down-sampled to a cubic source grid with voxels of 5 mm per side (Fig. 1E).',\n",
              "  '(4) VESTAL MEG source imaging used the source grid from step 3.',\n",
              "  'This step permits group-based analyses.',\n",
              "  \"In the shown example, MEG responses evoked by S1 localized to left and right Heschl's gyri (Fig. 1F).\",\n",
              "  '(5) Finally, for regions of interest (ROIs), the source time course was obtained by summing activity from all ROI voxels.',\n",
              "  \"Fig. 1H shows the time course from left Heschl's gyrus (dark blue region in Fig. 1C and D).\"],\n",
              " ['Another important finding is that, in spite of the immune responses mounted by the host brain, a substantial number of TH+ cells survived in the allografts.',\n",
              "  'This is consistent with previous clinical reports of human fetal cell transplantation.',\n",
              "  'Postmortem analyses of the patients revealed robust survival of DA neurons in spite of the fact that numerous immune cells were present around the graft (Kordower et al., 1997).',\n",
              "  'In two double-blind clinical trials, immunosuppressive drugs were never used (Freed et al., 2001) or were withdrawn after 6 months (Olanow et al., 2003).',\n",
              "  'In these cases, the cells from multiple fetuses were used without HLA matching, but more than 50,000 TH+ cells had survived after several years.',\n",
              "  'Our quantitative PCR (qPCR) study in vitro showed that the expression of MHC-I increased in response to IFN-γ, but the expression level was still 1/10 that of untreated monkey peripheral blood cells (Figure 2B).',\n",
              "  'The in vivo studies revealed that the serum level of IFN-γ increased at 2 months, and CD45+ cells (including CD8+ cells) accumulated in the allografts 3.5–4 months after the transplant.',\n",
              "  'On the other hand, the levels of INF-γ in the cerebrospinal fluid (CSF) and the levels of tumor necrosis factor α (TNF-α) in both the serum and CSF were below the limit of detection by ELISA (data not shown).',\n",
              "  'An immunofluorescence study did not reveal any apparent expression of MHC-I by the grafted cells (Figure S4A).',\n",
              "  'Therefore, it is possible that the immune response in the primate brain was not strong enough to reject all of the donor cells.',\n",
              "  'These findings closely correlate with the results of previous murine experiments (Hudson et al., 1994; Shinoda et al., 1995).',\n",
              "  'To apply our findings to a more clinically relevant setting, we investigated the expression of HLA-I during neural differentiation of human ESCs (hESCs) and iPSCs by qPCR (Figure S4D).',\n",
              "  'The expression level was 1/100 compared with that of human peripheral blood cells in both hESCs and iPSCs, and it was similarly elevated in response to IFN-γ.',\n",
              "  'It is difficult to precisely compare immunogenicity in monkeys with that in humans, but the low expression level of MHC-I by the donor cells may account for the mild rejection in both monkey and human neural transplantation.'],\n",
              " ['We next addressed whether hESC-derived trigeminal neurons can engraft in the adult mouse CNS and project toward their physiological target.',\n",
              "  'The trigeminal nuclei in the brainstem receive afferent innervation from the trigeminal sensory ganglion that is relayed to the contralateral thalamus.',\n",
              "  'The pons was selected as site for transplantation, because it is surgically accessible and located within proximity of the trigeminal brain stem nuclei that receive afferent input from the trigeminal ganglia.',\n",
              "  'Hence, GFP+ human trigeminal neuron clusters were injected into adult NOD/SCID mice via stereotactic surgery (see Experimental Procedures).',\n",
              "  'Histological analysis 4 weeks after transplantation showed survival of GFP+ human cell graft in the ventral pons (Figure S5E).',\n",
              "  'Although GFP+ cell bodies remained tightly clustered at injection site, GFP+ fibers showed extensive projections into the host brain (n = 6) including the endogenous trigeminal nuclei (Figure S5F).',\n",
              "  'Expression of BRN3A confirmed the sensory neuron identity of the cells (Figure S5G).',\n",
              "  'Graft-derived human fiber bundles (hNCAM+ and GFP+) were observed emanating from the graft core (Figure S5H).',\n",
              "  'These data demonstrate in vivo survival of trigeminal placode derivatives, differentiation along sensory neuron lineage, and the establishment of axonal projections toward relevant endogenous targets in the embryonic chick and adult mouse brain.'],\n",
              " ['(B) Fetal hRPE stained for SC121 (red) and MCT1 (green; scale bars, 125 μm and 25 μm [inset]).'],\n",
              " ['(F) Intracellular PGRN levels in iPSC-derived neurons after medium collection.',\n",
              "  'Values of control line 20 were set to 100% (n = 3–4 independent cultures).'],\n",
              " ['Prior to VESTAL analyses, a 5–55 Hz bandpass filter was applied.',\n",
              "  'VESTAL analyses examined activity 30–130 ms post-stimulus producing a 4D activation map (3D volumes across time) as well as a 2D source time-course matrix.',\n",
              "  'The average percent variance explained for gradiometer data using VESTAL was 95.81% for HC and 94.38% for SZ.',\n",
              "  'The average percent variance explained for magnetometer data using VESTAL program was 96.24% for HC and 93.17% for SZ.',\n",
              "  'There were no group differences in percent variance explained for gradiometer data (t(39) = 1.16, p = 0.25) or magnetometer data (t(39) = 1.31, p = 0.20).'],\n",
              " ['(C) Control lines 16 and 17, sporadic lines 12 and 23, and PGRN S116X lines 1 and 14 were immunostained for AFP (endoderm), desmin (mesoderm), and βIII-tubulin (ectoderm), and counterstained with DAPI (nuclei).',\n",
              "  'All lines showed a normal karyotype.',\n",
              "  'Scale bar: 50 μm.'],\n",
              " ['(I–M) Teratoma formation at 3 months after transplantation in the testes of SCID mice.',\n",
              "  'H&E staining of the sections showed histological features of the neuroepithelium (J), cartilage (K), muscle (L) and gut-like epithelium (M).'],\n",
              " ['Using a recombinant adeno-associated viral (rAAV)-based gene-targeting method, we inserted the gene-encoding GFP into the SOX2 locus in H9 hESCs (Figure 1A).',\n",
              "  'Proper homologous recombination led to the replacement of the SOX2 open reading frame with that of GFP and a neomycin selection cassette (SV40-Neo).',\n",
              "  'After infection with rAAV and G418 drug selection, a total of 36 clones were expanded and screened by Southern blotting for homologous recombination events.',\n",
              "  'Among these clones, 26 (72%) were found to carry the GFP-Neo cassette in the SOX2 locus (Figure S1A available online).',\n",
              "  'No clones in which both SOX2 alleles were disrupted were isolated.',\n",
              "  'Our subsequent analysis focused on one of these clones, clone 23 (hSOX2-23).',\n",
              "  'We confirmed appropriate gene targeting in this clone using multiple restriction digests followed by Southern blotting (Figures 1B, S1B, and S1C).',\n",
              "  'We did not observe nontargeted insertions of the rAAV sequences, and cells exhibited a normal karyotype (data not shown).',\n",
              "  'Flow cytometry of hSOX2-23 revealed that the majority of the cells expressed GFP (Figure 1C).',\n",
              "  'By comparison, a drug-selected clone, hSOX2-25, which was negative for targeted insertion (Figure S1A), showed no detectable GFP (Figure S2A).',\n",
              "  'Despite only having one copy of SOX2, hSOX2-23 had similar levels of SOX2, OCT4, and NANOG expression as hSOX2-25 and wild-type (WT) hESCs (Figure S2B).',\n",
              "  'Moreover, the percentage of GFP-positive (GFP+) cells in hSOX2-23 was constant over more than 20 passages.',\n",
              "  'Immunofluorescence (IF) staining of hSOX2-23 showed that 100% of GFP+ cells expressed SOX2 protein (Figure S2C).',\n",
              "  'Additionally, hSOX2-23 colonies had characteristic hESC morphology (Figure S2D) and expressed markers of the undifferentiated state, such as NANOG (Figure S2E).',\n",
              "  'These results show that this rAAV-based gene-targeting method can be used to efficiently disrupt genes by homologous recombination.',\n",
              "  'In addition, the SOX2-GFP hESC marker line can be used to monitor SOX2 expression in undifferentiated hESCs.'],\n",
              " ['Based on several previous publications (Khan et al., 2010, 2011), we explored the utility of adeno-associated virus (AAV) as a method to improve gene targeting efficiencies in hPSCs.',\n",
              "  'For SOX2, a gene that is highly expressed in undifferentiated hPSCs, gene targeting rates were greater than 70%.',\n",
              "  'Similar targeting efficiencies in hPSCs using AAV have been reported by others (Asuri et al., 2012; Khan et al., 2010, 2011; Smith-Arica et al., 2003), indicating that AAV offers a highly efficient and robust approach to target genes for HR in hPSCs.'],\n",
              " ['(C) C-peptide and PDX1 expression was confirmed by immunocytochemistry of cells differentiated for 25 days.'],\n",
              " ['•Adult hRPESC-derived RPE had comparable in vitro characteristics to fetal hRPE•hRPE monolayers survived 4 weeks on PET carriers under the rabbit retina•Better xenograft survival may be due to the maintained hRPE cell polarity•Atrophy of the retina overlaying the hRPE xenograft remains a future challenge'],\n",
              " ['(O) GH plasma levels using a human specific ELISA (6 weeks after transplantation).'],\n",
              " ['(R) Quantification of immunocytochemical analyses for each iPSC line.',\n",
              "  'Data are shown as the means ± SD (n = 3 independent experiments).',\n",
              "  'SER, serotonin, TUBβIII, β-tubulin class III.',\n",
              "  'Scale bars: 200 μm in (A)–(H), 50 μm in insets of (C)–(F), 100 μm in (J)–(M) and (Q).'],\n",
              " ['The two FTD patients under investigation in this study were part of a longitudinal dementia research program at the Memory and Aging Center, University of California, San Francisco.',\n",
              "  'Both had an 8-year history of behavioral changes and memory impairment at the time of tissue collection for this study.',\n",
              "  'One patient, a 67-year-old male with sporadic FTD, tested negative for mutations in GRN, MAPT, and C9ORF72.',\n",
              "  'The other patient, a 64-year-old male with a significant family history of dementia, had behavioral variant FTD.',\n",
              "  'MRI in this patient demonstrated severe bifrontal and temporal atrophy associated with gliosis in the frontal lobes (greater on the right).',\n",
              "  'One year later, MRI scans showed progression of atrophy and gliosis.',\n",
              "  'Genetic testing revealed a novel nonsense mutation in GRN, p.S116X (g.4627C > A, c.347C > A), which is predicted to result in a premature stop codon.',\n",
              "  'Both FTD patients had parkinsonism, which is typical of all FTD patients with PGRN mutations.',\n",
              "  'An age-matched subject, a clinically normal 64-year-old male with no mutations in GRN, MAPT, or C9ORF72, served as a control.'],\n",
              " ['(Q–S) Magnetic resonance images of a representative animal (No. 6, autograft) at 3 months after the transplant.',\n",
              "  'The arrowheads indicate the directions of the cell injections.',\n",
              "  '(Q) coronal, (R) axial, and (S) sagittal.',\n",
              "  'The letter L indicates the left side.'],\n",
              " ['(A) Fetal hRPE stained for pan-cytokeratin (scale bar, 50 μm); inset shows section overview stained with hematoxylin/eosin (scale bar, 200 μm).'],\n",
              " ['To assess the in vivo properties of hESC-derived trigeminal placode precursors, PIP-induced neuronal clusters, derived from a constitutively GFP-positive hESC line (Figures S5A and S5B), were injected into the developing chick embryo targeting the early trigeminal anlage at H&H stage 10–12 (Figure S5C).',\n",
              "  'Human cells were identified based on GFP expression and use of human specific antibodies against cytoplasmic antigen (hCA).',\n",
              "  'Two days after in ovo transplantation, surviving GFP+ cells were found dispersed in the area of the endogenous chick trigeminal ganglion (Figure 4P).',\n",
              "  'We observed extensive GFP+ human fiber bundles coexpressing hCA and peripherin (Figures 4Q and 4R).',\n",
              "  'In contrast, no hCA or peripherin expression was detected in the neural tube of the embryo (Figure S5D).',\n",
              "  'The in vivo fiber outgrowth 2 days after transplantation was reminiscent of the extensive in vitro fiber outgrowth of replated trigeminal neuron clusters (Figure S5A).',\n",
              "  'Peripherin expression in vivo (Figure 4S) confirmed the peripheral neuron identity of the grafted cells.'],\n",
              " ['For the first two animals (Nos. 1 and 4), we established iPSCs from fibroblasts derived from the oral mucosa using retroviral vectors (Okita et al., 2011).',\n",
              "  'For the other two animals (Nos. 6 and 8), we used peripheral blood mononuclear cells (PBMCs) with nonintegrating episomal vectors (Okita et al., 2013).',\n",
              "  'We selected the best clone from each animal according to the following criteria: a stable embryonic stem cell (ESC)-like morphology of the colonies after passaging, expression of pluripotent markers, few or no integrated transgenes (Figures 1A–1F; Figure S1 available online), and the potential for stable neural differentiation.',\n",
              "  'A PCR analysis revealed that all of the clones with retroviral vectors showed apparent expression of remaining transgenes (Figures S1C and S1D), whereas the clones with episomal vectors never did (Figure S1F).',\n",
              "  'To detect the iPSC-derived cells in a brain, we introduced GFP (Figures 1G and 1H).',\n",
              "  'The selected clones of iPSCs had the potential to generate teratomas in the testes of a severe combined immunodeficiency (SCID) mouse within 12 weeks (Figures 1I–1M).'],\n",
              " ['(A) Flow-cytometric analyses for MHC-I (HLA-A, HLA-B, and HLA-C).',\n",
              "  'Incubation of the cells with IFN-γ for 48 hr increased the MHC-I expression (green).'],\n",
              " ['Our group has developed a defined culture system to direct the differentiation of hPSCs into a near-homogenous population of definitive endoderm (DE) cells that have the capacity to differentiate into hepatocytes and pancreatic progenitors (Brown et al., 2011; Cho et al., 2012; Rashid et al., 2010; Touboul et al., 2010; Vallier et al., 2009a; Yusa et al., 2011).',\n",
              "  'Cells grown in these culture conditions successively express primitive streak markers (T and Mixl1), downregulate pluripotency markers (NANOG, SOX2, and POU5F1) and progressively upregulate definitive endoderm markers (CXCR4, FOXA2, GATA4, CERB, and SOX17) (Figures S1A–S1C available online).',\n",
              "  'Flow cytometry analyses showed that 80% of the resulting DE population coexpresses CXCR4 and SOX17 (Figure S1D).',\n",
              "  'Interestingly, the resulting population of DE cells is negative for genes marking the foregut (SOX2), the midgut/hindgut (CDX2), the pancreas (PDX1), the liver (AFP), and the lungs (HOXA1) (Figures S1E and S1F) This confirms that DE cells generated in vitro could correspond to early endoderm progenitor cells prior to anteroposterior patterning or organogenesis.'],\n",
              " ['In sequential PET studies, we observed increased uptake of [11C]PK11195 in one allograft (animal No. 10) at 3 months (Figures 3A and 3B).',\n",
              "  'We could not detect any apparent uptake in the other animals or at any other time points (Figure S2).',\n",
              "  'Intriguingly, the serum level of IFN-γ temporarily increased at 2 months after the transplant in three animals (Figure 3C).',\n",
              "  'An immunofluorescence study conducted at 3.5–4 months showed that MHC-II+ cells were more frequently found in allografts than in autografts, especially in the monkey with increased uptake of [11C]PK11195 (Figure 3D, No. 10).',\n",
              "  'The MHC-II staining never overlapped with that of GFP of the donor cells (Figure 3F), whereas it generally overlapped with that of IBA1 (Figure 3G), indicating that MHC-II was expressed by host-derived microglia.',\n",
              "  'Consistently, the number and density of IBA1+ cells were higher in allografts than in autografts (Figures 3E, 3H, and S4C).',\n",
              "  'An increase in the expression of MHC might trigger the recruitment of circulating immune cells, including T cells.',\n",
              "  'An immunofluorescence study revealed that more CD45+ cells (a marker for pan-leukocytes) accumulated in allografts compared with autografts (Figures 3I and 3J).',\n",
              "  'Most of the CD45+ cells were CD3+ T cells, and 60% of them were CD8+ killer T cells (Figures 3K and 3L).',\n",
              "  'These findings suggest that an acquired immune response was elicited only in the allografts in the primate brain.'],\n",
              " ['This study was approved by the Institutional Review Board and Ethics Committees of the University of California, San Francisco, and written informed consent was obtained in all cases.',\n",
              "  'The patient with the PGRN S116X mutation followed the classic clinical progression for FTD and developed parkinsonism, as do all FTD patients with PGRN mutations, but he did not show typical features of PD dementia.',\n",
              "  'The patient with sporadic FTD also showed parkinsonism.',\n",
              "  'Skin biopsies were collected, cut into small pieces, and placed on culture dishes to allow the fibroblasts to expand.',\n",
              "  \"The cells were maintained in Dulbecco's modified Eagle's medium supplemented with 10% fetal bovine serum, 1X nonessential amino acids, and penicillin/streptomycin (100 U/ml). iPSCs were generated as described previously (Takahashi et al., 2007).\",\n",
              "  'Please see Supplemental Information for more details.'],\n",
              " ['(D–H) Adult hRPE stained for SC121 (red).',\n",
              "  '(D) Adult hRPE stained for MCT1 (green).',\n",
              "  '(E) Human adult RPE stained for ezrin (green). hRPEs transplanted into rabbit SRS show absence of expression of ki67 (F), phosphohistone H3 (G), and caspase-3 (H).',\n",
              "  'Polarized fetal and adult hRPE cells were found in TEM (I and J).',\n",
              "  'Nuclei with regular chromatin were found in the basal compartment, a basal lamina ([I], large black arrowhead) had formed between the xenograft and PET carrier (black asterisks).',\n",
              "  'Melanosomes (M) in multiple stages, some microvilli abutting to the atrophic neural retina (NR), and junctional structures with desmosomes (small black arrowhead) and tight junctions (red arrowhead) were discerned apically.',\n",
              "  'Mitochondria (MC) were seen in the basolateral part of the cell.',\n",
              "  'Detachment from cell carrier (asterisk) in (J) is a histologic processing artifact.',\n",
              "  'Left images in (I) and (J) taken at 10,500× magnification; right micrographs are rectangular zone in left at 25,000×; scale bars represent 2 μm/inset 0.2 μm distance in (I) and (J).'],\n",
              " ['iPSC colonies were detached with accutase (Millipore) and grown as embryoid bodies (EBs) in suspension for 5–6 days in iPSC medium without basic fibroblast growth factor.',\n",
              "  'EBs were allowed to attach and form rosettes.',\n",
              "  'Ten-day-old rosettes were collected and grown in suspension as neurospheres.',\n",
              "  'Neurospheres were dissociated after 3–4 weeks, and the cells were plated on glass coverslips (BD Biosciences) or plates coated with poly-D-lysine (0.1 mg/ml) and laminin (10 μg/m).',\n",
              "  'Neurons were used after 2-4 weeks in culture.'],\n",
              " ['For cell differentiation, FAP-specific iPS cells were cultured with serial changes of media as shown in Fig. 3A.',\n",
              "  'To test whether FAP-specific iPS cells can differentiate into hepatocyte-like cells, we analyzed several markers via real-time PCR analysis on Day 5 (D5), D13 and D20 differentiated FAP-specific iPS cells (Fig. 3B).',\n",
              "  'A decrease in expression of the pluripotency marker Oct3/4 was accompanied by differentiation of FAP-specific iPS cells.',\n",
              "  'Expression of the endoderm marker Sox17 was observed on D5 differentiation and decreased gradually after the medium was changed to hepatic differentiation medium on D7 (Fig. 3B).',\n",
              "  'The hepatic progenitor marker AFP and the mature hepatocyte marker ALB were obviously expressed on D13 and D20.',\n",
              "  'In addition, immunocytochemical analyses showed Sox17 expression on D5, both HNF-4α and AFP expression on D13, and ALB cytoplasmic staining on D20 (Fig. 3C).',\n",
              "  'Quantitative imaging analysis revealed that approximately 78 ± 0.6% of cells were Sox17-positive on D5 and approximately 88 ± 1.1% of cells were AFP-positive on D13 and approximately 29 ± 0.9% of cells were ALB-positive on D20 (Fig. 3D).',\n",
              "  'The ALB secretion in the media of differentiated FAP-specific iPS cells on D20 was approximately 20 μg/ml (Fig. 3E).',\n",
              "  'Moreover, these D20 differentiated FAP-specific iPS cells were also periodic acid-Schiff (PAS)-positive, indicating cytoplasmic glycogen storage (Fig. 3F).',\n",
              "  'These results clearly indicated that FAP-specific iPS cells had the potential to differentiate into hepatocyte-like cells.']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3brnsvJ5BWAd"
      },
      "source": [
        "class BERT_Arch(nn.Module):\r\n",
        "    def __init__(self, bert, embed_dim, hidden_dim, drop_prob, n_layers, out_dim):\r\n",
        "      \r\n",
        "      super(BERT_Arch, self).__init__()\r\n",
        "\r\n",
        "      self.bert = bert \r\n",
        "      self.bilstm = nn.LSTM(embed_dim, hidden_dim,  bidirectional=True, batch_first=True)\r\n",
        "      self.dropout = nn.Dropout(drop_prob)\r\n",
        "      self.fc1 = nn.Linear(2*hidden_dim,out_dim)\r\n",
        "      self.sigmoid = nn.Sigmoid()\r\n",
        "\r\n",
        "    #define the forward pass\r\n",
        "    def forward(self, sent_id, mask):\r\n",
        "\r\n",
        "      #pass the inputs to the model  \r\n",
        "      x = self.bert(sent_id, attention_mask=mask)\r\n",
        "      embed = x[0]\r\n",
        "      cls_hs = x[1]\r\n",
        "      x,_ = self.bilstm(embed)\r\n",
        "      x = self.dropout(x)\r\n",
        "      x = self.fc1(x)\r\n",
        "      x = self.sigmoid(x)\r\n",
        "\r\n",
        "      return x\r\n",
        "      \r\n",
        "bert_model = BERT_Arch(model_BioBert, 768, 256, 0.5, 1,1)\r\n",
        "bert_model = bert_model.to(device)\r\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32OjC-RMEO7Z"
      },
      "source": [
        "Measured Property"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxgzE2oCBc8Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8feae492-bb93-49cb-99e7-c68ed3aa2353"
      },
      "source": [
        "bert_model = torch.load(MEASURED_PROPERTY_MODEL_PATH)\r\n",
        "Measured_Property=[]\r\n",
        "\r\n",
        "for paragraph in test_text:\r\n",
        "  para=[]\r\n",
        "  for s in paragraph:\r\n",
        "    tokenized_text = tokenizer_BioBert.tokenize(s)\r\n",
        "    test_inputs = tokenizer_BioBert(s,max_length = 512,padding='max_length',truncation=True, return_tensors=\"pt\")\r\n",
        "    id=test_inputs['input_ids'][0].numpy()\r\n",
        "    bert_model.eval()\r\n",
        "    temp = bert_model(test_inputs['input_ids'].to(device), test_inputs['attention_mask'].to(device))>=0.30\r\n",
        "    \r\n",
        "    \r\n",
        "    b = []\r\n",
        "    i=1\r\n",
        "    for m in re.finditer(r'[.,\\S]+', s):\r\n",
        "      w = m.group(0)\r\n",
        "      t = (tokenizer_BioBert.encode(w, add_special_tokens=False), (m.start(), m.end()-1))\r\n",
        "      vec=t[0]\r\n",
        "      \r\n",
        "      for j in vec:\r\n",
        "        b.append(t[1])\r\n",
        "        \r\n",
        "    \r\n",
        "    \r\n",
        "    txt=[]\r\n",
        "    span=np.zeros(len(s))\r\n",
        "    \r\n",
        "    for i in range(len(tokenized_text)):\r\n",
        "      \r\n",
        "      if(temp[0][i+1]):\r\n",
        "        txt.append(tokenized_text[i])\r\n",
        "        if(len(b[i])!=0):\r\n",
        "          span[b[i][0]:b[i][1]+1]=1\r\n",
        "        \r\n",
        "        \r\n",
        "    start=-1\r\n",
        "    end=-1\r\n",
        "    new_span=[]\r\n",
        "    for i in range(len(s)):\r\n",
        "      if(span[i]==1):\r\n",
        "        if(start==-1):\r\n",
        "          start=i\r\n",
        "          end=i\r\n",
        "        else:\r\n",
        "          end=i\r\n",
        "      else:\r\n",
        "        if(start==-1):\r\n",
        "          end=-1\r\n",
        "          continue\r\n",
        "        elif(span[i-1]==1 and span[i+1]==1):\r\n",
        "          end=i;\r\n",
        "        else:\r\n",
        "          new_span.append([start,end])\r\n",
        "          start=-1\r\n",
        "          end=-1\r\n",
        "\r\n",
        "    if(start!=-1 and end!=-1):\r\n",
        "        new_span.append([start,end])\r\n",
        "    answer=[]    \r\n",
        "    for v in new_span:\r\n",
        "      \r\n",
        "      answer.append([s[v[0]:v[1]+1],v[0],v[1]])\r\n",
        "    \r\n",
        "    para.append(answer)\r\n",
        "  print(para)\r\n",
        "  Measured_Property.append(para)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[], [], [], [], [['scale bars', 4, 13]]]\n",
            "[[], [], [], []]\n",
            "[[], [], [], [['with voxels', 64, 74], ['side', 88, 91]], [], [], [], [], []]\n",
            "[[], [], [], [['used', 72, 75], ['withdrawn', 106, 114]], [['TH+', 101, 103], ['survived', 115, 122]], [['expression', 59, 68], ['MHC-I increased', 73, 87], ['response', 92, 99], ['IFN-γ,', 104, 109], ['expression level', 119, 134]], [['IFN-γ increased', 53, 67]], [], [], [], [], [], [['The expression level', 0, 19]], []]\n",
            "[[], [], [], [], [], [], [], [], []]\n",
            "[[]]\n",
            "[[], [['Values', 0, 5], ['20', 23, 24]]]\n",
            "[[], [], [], [['percent variance', 12, 27]], []]\n",
            "[[['counterstained', 176, 189]], [], [['Scale bar:', 0, 9]]]\n",
            "[[], []]\n",
            "[[], [], [], [['clones,', 12, 18], ['found', 34, 38], ['cassette', 61, 68]], [], [], [], [], [], [['GFP', 125, 127]], [], [['GFP-positive (GFP+) cells', 28, 52], ['constant', 70, 77]], [['GFP+', 65, 68]], [], [], []]\n",
            "[[], [['gene targeting rates', 69, 88]], []]\n",
            "[[['immunocytochemistry', 51, 69], ['cells differentiated', 74, 93]]]\n",
            "[[['survived', 96, 103]]]\n",
            "[[]]\n",
            "[[], [], [['TUBβIII, β-tubulin', 16, 33]], []]\n",
            "[[], [], [['tested negative', 51, 65]], [], [], [], [], [], []]\n",
            "[[], [], [], []]\n",
            "[[['stained for pan-cytokeratin', 15, 41]]]\n",
            "[[], [], [], [], [], [['fiber outgrowth', 12, 26]], []]\n",
            "[[], [], [], [], [], []]\n",
            "[[], [['IFN-γ', 29, 33]]]\n",
            "[[], [], [], []]\n",
            "[[['uptake', 49, 54]], [['uptake', 33, 38]], [['IFN-γ', 33, 37], ['increased', 51, 59]], [], [], [], [], [], [['CD3+', 29, 32], ['CD8+', 64, 67], ['T', 76, 76]], []]\n",
            "[[], [], [], [], [['nonessential amino acids, and penicillin/streptomycin', 109, 161]], []]\n",
            "[[], [], [], [], [], [], [], [], [['magnification;', 44, 57], ['distance', 157, 164]]]\n",
            "[[['suspension', 92, 101]], [], [], [['dissociated', 18, 28]], []]\n",
            "[[], [], [], [['endoderm marker Sox17', 18, 38]], [], [['Sox17', 48, 52], ['ALB cytoplasmic staining', 115, 138]], [['were Sox17-positive', 77, 95], ['AFP-positive', 145, 156], ['ALB-positive', 207, 218]], [['secretion', 8, 16], ['differentiated FAP-specific iPS cells on D20', 34, 77]], [], []]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "345-i1CqD6zv"
      },
      "source": [
        "Measured Entity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yaau-0i4DjHP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63299df3-41ca-47e6-b837-ca4ed81d0463"
      },
      "source": [
        "bert_model = torch.load(MEASURED_ENTITY_MODEL_PATH)\r\n",
        "Measured_Entity=[]\r\n",
        "for paragraph in test_text:\r\n",
        "  para=[]\r\n",
        "  for s in paragraph:\r\n",
        "    tokenized_text = tokenizer_BioBert.tokenize(s)\r\n",
        "    test_inputs = tokenizer_BioBert(s,max_length = 512,padding='max_length',truncation=True, return_tensors=\"pt\")\r\n",
        "    id=test_inputs['input_ids'][0].numpy()\r\n",
        "    bert_model.eval()\r\n",
        "    temp = bert_model(test_inputs['input_ids'].to(device), test_inputs['attention_mask'].to(device))>=0.30\r\n",
        "    \r\n",
        "    \r\n",
        "    b = []\r\n",
        "    i=1\r\n",
        "    for m in re.finditer(r'[.,\\S]+', s):\r\n",
        "      w = m.group(0)\r\n",
        "      t = (tokenizer_BioBert.encode(w, add_special_tokens=False), (m.start(), m.end()-1))\r\n",
        "      vec=t[0]\r\n",
        "      \r\n",
        "      for j in vec:\r\n",
        "        b.append(t[1])\r\n",
        "        \r\n",
        "    \r\n",
        "    \r\n",
        "    txt=[]\r\n",
        "    span=np.zeros(len(s))\r\n",
        "    \r\n",
        "    for i in range(len(tokenized_text)):\r\n",
        "      \r\n",
        "      if(temp[0][i+1]):\r\n",
        "        txt.append(tokenized_text[i])\r\n",
        "        if(len(b[i])!=0):\r\n",
        "          span[b[i][0]:b[i][1]+1]=1\r\n",
        "        \r\n",
        "        \r\n",
        "    start=-1\r\n",
        "    end=-1\r\n",
        "    new_span=[]\r\n",
        "    for i in range(len(s)):\r\n",
        "      if(span[i]==1):\r\n",
        "        if(start==-1):\r\n",
        "          start=i\r\n",
        "          end=i\r\n",
        "        else:\r\n",
        "          end=i\r\n",
        "      else:\r\n",
        "        if(start==-1):\r\n",
        "          end=-1\r\n",
        "          continue\r\n",
        "        elif(span[i-1]==1 and span[i+1]==1):\r\n",
        "          end=i;\r\n",
        "        else:\r\n",
        "          new_span.append([start,end])\r\n",
        "          start=-1\r\n",
        "          end=-1\r\n",
        "\r\n",
        "    if(start!=-1 and end!=-1):\r\n",
        "        new_span.append([start,end])\r\n",
        "    answer=[]    \r\n",
        "    for v in new_span:\r\n",
        "      \r\n",
        "      answer.append([s[v[0]:v[1]+1],v[0],v[1]])\r\n",
        "    para.append(answer)\r\n",
        "  print(para)\r\n",
        "  Measured_Entity.append(para)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[], [], [], [], [['scale bars', 4, 13], ['μm.', 28, 30]]]\n",
            "[[['GFP-expressing H9,', 0, 17]], [], [['GFP cell', 25, 32]], [['Wells', 0, 4]]]\n",
            "[[], [['T1-weighted sMRIs', 4, 20]], [['regions', 319, 325], ['regions in', 331, 340], ['hemisphere),', 347, 358]], [['regional masks', 8, 21], ['cubic source grid', 46, 62], ['voxels', 69, 74]], [], [], [['S1', 46, 47]], [['regions', 17, 23], ['interest (ROIs),', 28, 43]], []]\n",
            "[[], [], [], [['double-blind clinical trials, immunosuppressive', 7, 53]], [['TH+', 101, 103]], [['MHC-I', 73, 77], ['untreated monkey peripheral blood cells', 159, 197]], [['IFN-γ', 53, 57]], [], [], [], [], [], [['expression', 4, 13], ['peripheral blood cells', 59, 80], ['hESCs and iPSCs,', 90, 105]], []]\n",
            "[[['trigeminal neurons', 39, 56]], [['trigeminal nuclei', 4, 20]], [['pons', 4, 7]], [['GFP+ human trigeminal neuron clusters', 7, 43]], [], [['trigeminal', 166, 175]], [], [], [['trigeminal', 43, 52]]]\n",
            "[[['Fetal hRPE', 4, 13]]]\n",
            "[[], [['Values', 0, 5], ['control line 20', 10, 24], ['cultures).', 64, 73]]]\n",
            "[[['bandpass filter', 36, 50]], [['VESTAL', 0, 5], ['volumes', 92, 98]], [['HC', 88, 89], ['SZ.', 106, 108]], [['VESTAL', 67, 72], ['HC', 97, 98], ['SZ.', 115, 117]], [['no group differences', 11, 30], ['percent variance explained', 35, 60], ['gradiometer', 66, 76], ['(t(39)', 83, 88], ['magnetometer data', 111, 127]]]\n",
            "[[['Control lines', 4, 16]], [['lines', 4, 8]], [['Scale bar:', 0, 9]]]\n",
            "[[], []]\n",
            "[[['H9 hESCs', 136, 143]], [], [['36 clones', 62, 70]], [['clones,', 12, 18]], [], [['these clones, clone 23 (hSOX2-23).', 42, 75]], [], [], [['hSOX2-23', 18, 25], ['cells', 61, 65]], [['drug-selected clone, hSOX2-25,', 17, 46]], [['of SOX2, hSOX2-23', 29, 45], ['hSOX2-25 and wild-type', 105, 126]], [['GFP-positive (GFP+) cells in hSOX2-23', 28, 64]], [['Immunofluorescence', 0, 17], ['hSOX2-23', 36, 43], ['GFP+ cells', 65, 74]], [['hSOX2-23', 14, 21]], [], []]\n",
            "[[], [], []]\n",
            "[[]]\n",
            "[[['RPE', 22, 24], ['hRPE•hRPE monolayers', 75, 94]]]\n",
            "[[]]\n",
            "[[], [['Data', 0, 3], ['3 independent experiments).', 38, 64]], [], [['bars:', 6, 10], ['(A)–(H),', 22, 29], ['insets of', 40, 48]]]\n",
            "[[['FTD patients', 8, 19]], [], [['patient,', 4, 11]], [['patient,', 10, 17]], [], [], [], [['FTD', 5, 7]], [['subject,', 15, 22]]]\n",
            "[[['representative animal (No.', 37, 62]], [], [], []]\n",
            "[[['Fetal hRPE stained', 4, 21]]]\n",
            "[[['neuronal clusters,', 92, 109], ['trigeminal anlage', 254, 270]], [['cells', 6, 10]], [['transplantation,', 22, 37], ['GFP+', 49, 52]], [], [], [], []]\n",
            "[[['animals', 18, 24]], [['animals', 18, 24]], [], [['clones', 40, 45]], [], [['immunodeficiency (SCID)', 104, 126]]]\n",
            "[[], [['cells with IFN-γ', 18, 33]]]\n",
            "[[['definitive endoderm', 125, 143]], [], [['Flow cytometry', 0, 13], ['resulting DE population', 47, 69]], []]\n",
            "[[['PET', 14, 16], ['[11C]PK11195', 59, 70], ['allograft (animal', 79, 95]], [], [['IFN-γ', 33, 37], ['animals', 103, 109]], [], [], [], [], [], [], []]\n",
            "[[], [['PGRN S116X', 21, 30]], [['sporadic FTD', 17, 28]], [], [['cells', 4, 8], [\"modified Eagle's medium supplemented\", 40, 75], ['fetal bovine serum,', 86, 104], ['nonessential amino acids,', 109, 133], ['penicillin/streptomycin', 139, 161]], []]\n",
            "[[], [], [['RPE', 16, 18]], [], [], [], [], [], [['Left images', 0, 10], ['right micrographs', 59, 75], ['μm/inset', 141, 148]]]\n",
            "[[], [], [['rosettes', 12, 19]], [], [['Neurons', 0, 6]]]\n",
            "[[], [['iPS cells', 195, 203]], [], [], [], [], [['Quantitative imaging', 0, 19], ['cells', 71, 75], ['Sox17-positive', 82, 95], ['cells', 134, 138], ['cells', 196, 200]], [], [], []]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIp0vLvDEag2"
      },
      "source": [
        "Quantity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvR3aM47ELAs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163,
          "referenced_widgets": [
            "182d05b10c5247dcb90f1f80e25958d1",
            "e40ac889416745ab9bab3ca7e8b3ee0d",
            "5130ac8f8b1e46cc90fe509c1cdc3450",
            "b3bfc30affbd44d2a88a781486da195f",
            "7385557a178e4eefbeeba50ff6e768f1",
            "ce7fec646b7245da8df3ae0ca65d70af",
            "d2ef31335bb24cff823b1f25e4ce331d",
            "bff4ecdc16b74bd7aac85a3fd062d821",
            "19724eded2ba4634b18121b0d3bd438e",
            "7880f840603a4743bcdc716107646c49",
            "598559f3b5b34ff9b86161cf107f069e",
            "854781d1312241d69bc7748c90d1e928",
            "223ed6946b00498592f05feb841b08cd",
            "69506ed91df842deac0b7fc80c8a9d61",
            "cb3f947f000846eeb1a3960cb4419441",
            "67f501ade1af4b2f8738752fbb9eeb6a",
            "53355b2d6a2d4dbe95811728e7a060d2",
            "691d976732514df0aaba310f6174ef7c",
            "dfed7028e9e142368ccd42b5c3e566da",
            "9ddaf6454d9641e6be3855ddb746e2b3",
            "84ae707bed1f4d16827a2aaa583473f7",
            "cc02570915bb45f28c1784d96ec71b79",
            "bde5b15b491a4b7cab5c3ec299be2339",
            "150c943c067f4d5a85f0ebb5f306c807"
          ]
        },
        "outputId": "32778ad6-1b74-4211-89a5-8e0be0d623e0"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\r\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\r\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "182d05b10c5247dcb90f1f80e25958d1",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "19724eded2ba4634b18121b0d3bd438e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "53355b2d6a2d4dbe95811728e7a060d2",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wnDuSYJc7Ts",
        "outputId": "6696a83c-675d-4ba4-96e3-060dda08d548"
      },
      "source": [
        "Measured_Quantity=[]\r\n",
        "bert_model = BERT_Arch(model, 768, 256, 0.5, 1,1)\r\n",
        "bert_model = bert_model.to(device)\r\n",
        "bert_model = torch.load(MEASURED_QUANTITY_MODEL_PATH)\r\n",
        "for paragraph in test_text:\r\n",
        "  para=[]\r\n",
        "  for s in paragraph:\r\n",
        "    tokenized_text = tokenizer.tokenize(s)\r\n",
        "    test_inputs = tokenizer(s,max_length = 512,padding='max_length',truncation=True, return_tensors=\"pt\")\r\n",
        "    id=test_inputs['input_ids'][0].numpy()\r\n",
        "    bert_model.eval()\r\n",
        "    temp = bert_model(test_inputs['input_ids'].to(device), test_inputs['attention_mask'].to(device))>=0.7\r\n",
        "    \r\n",
        "    \r\n",
        "    b = []\r\n",
        "    i=1\r\n",
        "    for m in re.finditer(r'[.,\\S]+', s):\r\n",
        "      w = m.group(0)\r\n",
        "      t = (tokenizer.encode(w, add_special_tokens=False), (m.start(), m.end()-1))\r\n",
        "      vec=t[0]\r\n",
        "      \r\n",
        "      for j in vec:\r\n",
        "        b.append(t[1])\r\n",
        "        \r\n",
        "    \r\n",
        "    \r\n",
        "    txt=[]\r\n",
        "    span=np.zeros(len(s))\r\n",
        "    \r\n",
        "    for i in range(len(tokenized_text)):\r\n",
        "      \r\n",
        "      if(temp[0][i+1]):\r\n",
        "        txt.append(tokenized_text[i])\r\n",
        "        if(len(b[i])!=0):\r\n",
        "          span[b[i][0]:b[i][1]+1]=1\r\n",
        "        \r\n",
        "        \r\n",
        "    start=-1\r\n",
        "    end=-1\r\n",
        "    new_span=[]\r\n",
        "    for i in range(len(s)):\r\n",
        "      if(span[i]==1):\r\n",
        "        if(start==-1):\r\n",
        "          start=i\r\n",
        "          end=i\r\n",
        "        else:\r\n",
        "          end=i\r\n",
        "      else:\r\n",
        "        if(start==-1):\r\n",
        "          end=-1\r\n",
        "          continue\r\n",
        "        elif(span[i-1]==1 and span[i+1]==1):\r\n",
        "          end=i;\r\n",
        "        else:\r\n",
        "          new_span.append([start,end])\r\n",
        "          start=-1\r\n",
        "          end=-1\r\n",
        "\r\n",
        "    if(start!=-1 and end!=-1):\r\n",
        "        new_span.append([start,end])\r\n",
        "    answer=[]    \r\n",
        "    for v in new_span:\r\n",
        "      \r\n",
        "      answer.append([s[v[0]:v[1]+1],v[0],v[1]])\r\n",
        "    \r\n",
        "    \r\n",
        "    para.append(answer)\r\n",
        "  print(para)\r\n",
        "  Measured_Quantity.append(para)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[], [], [], [], [['10 μm.', 25, 30]]]\n",
            "[[], [], [], [['12 hr', 30, 34]]]\n",
            "[[], [], [['96', 295, 296]], [['5 mm per side', 79, 91]], [], [], [], [], []]\n",
            "[[], [], [], [['two', 3, 5], ['6 months', 122, 129]], [['more than 50,000', 84, 99]], [['1/10 that', 146, 154]], [['2 months,', 72, 80], ['3.5–4 months', 151, 162]], [], [], [], [], [], [['1/100', 25, 29]], []]\n",
            "[[], [], [], [], [['4 weeks', 22, 28]], [], [], [], []]\n",
            "[[['125 μm', 68, 73], ['25 μm', 79, 83]]]\n",
            "[[], [['100%', 38, 41], ['3–4', 48, 50]]]\n",
            "[[['5–55 Hz', 28, 34]], [['30–130 ms', 34, 42], ['4D', 70, 71]], [['95.81%', 77, 82], ['94.38%', 95, 100]], [['96.24%', 86, 91], ['93.17%', 104, 109]], [['1.16,', 92, 96], ['0.25)', 102, 106], ['1.31,', 138, 142]]]\n",
            "[[], [], [['50 μm.', 11, 16]]]\n",
            "[[['3 months', 28, 35]], []]\n",
            "[[], [], [['36', 62, 63]], [['26', 20, 21]], [], [], [], [], [], [], [['one', 20, 22]], [['more than 20 passages.', 84, 105]], [['100%', 57, 60]], [], [], []]\n",
            "[[], [['than 70%.', 103, 111]], []]\n",
            "[[['25 days.', 99, 106]]]\n",
            "[[['4 weeks', 105, 111]]]\n",
            "[[['(6 weeks', 50, 57]]]\n",
            "[[], [['3', 38, 38]], [], [['200 μm', 12, 17], ['50 μm', 31, 35], ['100 μm', 59, 64]]]\n",
            "[[['two', 4, 6]], [['8-year', 12, 17]], [['67-year-old', 15, 25]], [['64-year-old', 21, 31]], [], [], [], [], [['64-year-old', 44, 54]]]\n",
            "[[['3 months', 81, 88]], [], [], []]\n",
            "[[['50 μm);', 55, 61], ['200 μm).', 135, 142]]]\n",
            "[[], [], [], [], [], [['2 days', 28, 33]], []]\n",
            "[[['two', 14, 16]], [['two', 14, 16]], [], [], [], [['12 weeks', 141, 148]]]\n",
            "[[], [['48 hr', 39, 43]]]\n",
            "[[], [], [['80%', 36, 38]], []]\n",
            "[[['3 months', 108, 115]], [], [['2 months', 64, 71], ['three', 97, 101]], [['3.5–4 months', 41, 52]], [], [], [], [], [['60%', 47, 49]], []]\n",
            "[[], [], [], [], [['10%', 82, 84], ['1X', 106, 107]], []]\n",
            "[[], [], [], [], [], [], [], [], [['10,500×', 36, 42], ['25,000×;', 109, 116], ['2 μm/inset 0.2 μm', 139, 155]]]\n",
            "[[['5–6 days', 107, 114]], [], [['Ten-day-old', 0, 10]], [['3–4 weeks,', 36, 45], ['(0.1 mg/ml)', 146, 156], ['(10 μg/m).', 170, 179]], [['after 2-4 weeks', 18, 32]]]\n",
            "[[], [], [], [], [], [], [['approximately 78 ± 0.6%', 44, 66], ['approximately 88 ± 1.1%', 107, 129], ['29 ± 0.9%', 183, 191]], [['approximately 20 μg/ml', 83, 104]], [], []]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKwv9_zmEeIQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33cc185f-f43f-46f1-e40b-3745e8c300af"
      },
      "source": [
        "Measured_Quantity=[]\r\n",
        "bert_model = BERT_Arch(model, 768, 256, 0.5, 1,1)\r\n",
        "bert_model = bert_model.to(device)\r\n",
        "bert_model = torch.load(MEASURED_QUANTITY_MODEL_PATH)\r\n",
        "for paragraph in test_text:\r\n",
        "  para=[]\r\n",
        "  for s in paragraph:\r\n",
        "    tokenized_text = tokenizer.tokenize(s)\r\n",
        "    test_inputs = tokenizer(s,max_length = 512,padding='max_length',truncation=True, return_tensors=\"pt\")\r\n",
        "    id=test_inputs['input_ids'][0].numpy()\r\n",
        "    bert_model.eval()\r\n",
        "    temp = bert_model(test_inputs['input_ids'].to(device), test_inputs['attention_mask'].to(device))>=0.55\r\n",
        "    \r\n",
        "    \r\n",
        "    b = {}\r\n",
        "    for m in re.finditer(r'[.,\\S]+', s):\r\n",
        "      w = m.group(0)\r\n",
        "      t = (tokenizer.encode(w, add_special_tokens=False), (m.start(), m.end()-1))\r\n",
        "      vec=t[0]\r\n",
        "      for j in vec:\r\n",
        "        if j in b:\r\n",
        "          b[j].append(t[1])\r\n",
        "        else:\r\n",
        "\r\n",
        "          b[j]=[t[1]]\r\n",
        "    \r\n",
        "    \r\n",
        "    quant={}\r\n",
        "    \r\n",
        "    \r\n",
        "    txt=[]\r\n",
        "    for i in range(len(temp[0])):\r\n",
        "      \r\n",
        "      if(temp[0][i]):\r\n",
        "        txt.append(tokenized_text[i-1])\r\n",
        "        if(id[i] in b):\r\n",
        "          vec=b[id[i]]\r\n",
        "        \r\n",
        "          for q in vec:\r\n",
        "            quant[q]=1\r\n",
        "    l=[]\r\n",
        "    span=np.zeros(len(s))\r\n",
        "    for key,val in quant.items():\r\n",
        "      start=key[0]\r\n",
        "      end=key[1]\r\n",
        "      span[start:end+1]=1\r\n",
        "    new_span=[]\r\n",
        "    \r\n",
        "    start=-1\r\n",
        "    end=-1\r\n",
        "    for i in range(len(s)):\r\n",
        "      if(span[i]==1):\r\n",
        "        if(start==-1):\r\n",
        "          start=i\r\n",
        "          end=i\r\n",
        "        else:\r\n",
        "          end=i\r\n",
        "      else:\r\n",
        "        if(start==-1):\r\n",
        "          end=-1\r\n",
        "          continue\r\n",
        "        elif(span[i-1]==1 and span[i+1]==1):\r\n",
        "          end=i;\r\n",
        "        else:\r\n",
        "          new_span.append([start,end])\r\n",
        "          start=-1\r\n",
        "          end=-1\r\n",
        "\r\n",
        "          \r\n",
        "    if(start!=-1 and end!=-1):\r\n",
        "        new_span.append([start,end])\r\n",
        "    answer=[]    \r\n",
        "    for v in new_span:\r\n",
        "      \r\n",
        "      answer.append([s[v[0]:v[1]+1],v[0],v[1]])\r\n",
        "    \r\n",
        "    \r\n",
        "    para.append(answer)\r\n",
        "  print(para)\r\n",
        "  Measured_Quantity.append(para)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[], [], [], [], [['10 μm.', 25, 30]]]\n",
            "[[], [], [], [['12 hr', 30, 34]]]\n",
            "[[], [], [['96', 295, 296]], [['5 mm per side', 79, 91]], [], [], [], [], []]\n",
            "[[], [], [], [['two', 3, 5], ['after 6 months', 116, 129]], [['cases,', 9, 14], ['matching,', 70, 78], ['more than 50,000', 84, 99]], [['that', 50, 53], ['1/10 that', 146, 154]], [['2 months,', 72, 80], ['3.5–4 months', 151, 162], ['transplant.', 174, 184]], [], [], [], [], [], [['1/100', 25, 29]], []]\n",
            "[[], [], [], [], [['4 weeks', 22, 28]], [], [], [], []]\n",
            "[[['125 μm', 68, 73], ['25 μm', 79, 83]]]\n",
            "[[], [['100%', 38, 41], ['3–4', 48, 50]]]\n",
            "[[['5–55 Hz', 28, 34]], [['30–130 ms', 34, 42], ['4D', 70, 71]], [['95.81%', 77, 82], ['94.38%', 95, 100], ['SZ.', 106, 108]], [['96.24%', 86, 91], ['93.17%', 104, 109], ['SZ.', 115, 117]], [['(t(39)', 83, 88], ['1.16,', 92, 96], ['0.25)', 102, 106], ['(t(39)', 129, 134], ['1.31,', 138, 142], ['0.20).', 148, 153]]]\n",
            "[[], [], [['50 μm.', 11, 16]]]\n",
            "[[['3 months', 28, 35]], []]\n",
            "[[], [], [['36', 62, 63]], [['26', 20, 21]], [], [], [], [], [], [], [['one', 20, 22]], [['more than 20 passages.', 84, 105]], [['100%', 57, 60]], [], [], []]\n",
            "[[], [['than 70%.', 103, 111]], []]\n",
            "[[['25 days.', 99, 106]]]\n",
            "[[['4 weeks', 105, 111]]]\n",
            "[[['(6 weeks', 50, 57]]]\n",
            "[[], [['3', 38, 38]], [], [['200 μm', 12, 17], ['50 μm', 31, 35], ['100 μm', 59, 64]]]\n",
            "[[['two', 4, 6]], [['8-year', 12, 17]], [['67-year-old', 15, 25]], [['64-year-old', 21, 31]], [], [], [], [], [['age-matched', 3, 13], ['64-year-old', 44, 54]]]\n",
            "[[['3 months', 81, 88]], [], [], []]\n",
            "[[['(A)', 0, 2], ['50 μm);', 55, 61], ['200 μm).', 135, 142]]]\n",
            "[[], [], [], [], [], [['2 days', 28, 33]], []]\n",
            "[[['two', 14, 16]], [['two', 14, 16]], [], [], [], [['12 weeks', 141, 148]]]\n",
            "[[], [['48 hr', 39, 43]]]\n",
            "[[], [], [['80%', 36, 38]], []]\n",
            "[[['3 months', 108, 115], ['3B).', 133, 136]], [], [['2 months', 64, 71], ['three', 97, 101]], [['3.5–4 months', 41, 52], ['No. 10).', 216, 223]], [], [], [], [], [['60%', 47, 49]], []]\n",
            "[[], [], [], [], [['10%', 82, 84], ['1X', 106, 107]], []]\n",
            "[[], [], [], [], [], [], [], [], [['10,500×', 36, 42], ['25,000×;', 109, 116], ['2 μm/inset 0.2 μm', 139, 155], ['(J).', 177, 180]]]\n",
            "[[['5–6 days', 107, 114]], [], [['Ten-day-old', 0, 10]], [['3–4 weeks,', 36, 45], ['Biosciences)', 97, 108], ['(0.1 mg/ml)', 146, 156], ['(10 μg/m).', 170, 179]], [['after 2-4 weeks', 18, 32]]]\n",
            "[[], [], [], [], [], [], [['approximately 78 ± 0.6%', 44, 66], ['approximately 88 ± 1.1%', 107, 129], ['approximately 29 ± 0.9%', 169, 191], ['(Fig. 3D).', 227, 236]], [['approximately 20 μg/ml', 83, 104]], [], []]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBoVMN2SLMxF"
      },
      "source": [
        "Qualifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aY6HM7YG8Rn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6206b1b7-7171-4b39-fdcf-74837204237e"
      },
      "source": [
        "bert_model = BERT_Arch(model, 768, 256, 0.5, 1,1)\r\n",
        "bert_model = bert_model.to(device)\r\n",
        "bert_model = torch.load(QUALIFIER_PATH)\r\n",
        "Qual=[]\r\n",
        "for paragraph in test_text:\r\n",
        "  para=[]\r\n",
        "  for s in paragraph:\r\n",
        "    tokenized_text = tokenizer.tokenize(s)\r\n",
        "    test_inputs = tokenizer(s,max_length = 512,padding='max_length',truncation=True, return_tensors=\"pt\")\r\n",
        "    id=test_inputs['input_ids'][0].numpy()\r\n",
        "    bert_model.eval()\r\n",
        "    temp = bert_model(test_inputs['input_ids'].to(device), test_inputs['attention_mask'].to(device))>=0.995\r\n",
        "    \r\n",
        "    \r\n",
        "    b = {}\r\n",
        "    for m in re.finditer(r'[.,\\S]+', s):\r\n",
        "      w = m.group(0)\r\n",
        "      t = (tokenizer.encode(w, add_special_tokens=False), (m.start(), m.end()-1))\r\n",
        "      vec=t[0]\r\n",
        "      for j in vec:\r\n",
        "        if j in b:\r\n",
        "          b[j].append(t[1])\r\n",
        "        else:\r\n",
        "\r\n",
        "          b[j]=[t[1]]\r\n",
        "    \r\n",
        "    \r\n",
        "    quant={}\r\n",
        "    \r\n",
        "    \r\n",
        "    txt=[]\r\n",
        "    for i in range(len(temp[0])):\r\n",
        "      \r\n",
        "      if(temp[0][i]):\r\n",
        "        txt.append(tokenized_text[i-1])\r\n",
        "        if(id[i] in b):\r\n",
        "          vec=b[id[i]]\r\n",
        "        \r\n",
        "          for q in vec:\r\n",
        "            quant[q]=1\r\n",
        "    l=[]\r\n",
        "    span=np.zeros(len(s))\r\n",
        "    for key,val in quant.items():\r\n",
        "      start=key[0]\r\n",
        "      end=key[1]\r\n",
        "      span[start:end+1]=1\r\n",
        "    new_span=[]\r\n",
        "    \r\n",
        "    start=-1\r\n",
        "    end=-1\r\n",
        "    for i in range(len(s)):\r\n",
        "      if(span[i]==1):\r\n",
        "        if(start==-1):\r\n",
        "          start=i\r\n",
        "          end=i\r\n",
        "        else:\r\n",
        "          end=i\r\n",
        "      else:\r\n",
        "        if(start==-1):\r\n",
        "          end=-1\r\n",
        "          continue\r\n",
        "        elif(span[i-1]==1 and span[i+1]==1):\r\n",
        "          end=i;\r\n",
        "        else:\r\n",
        "          new_span.append([start,end])\r\n",
        "          start=-1\r\n",
        "          end=-1\r\n",
        "\r\n",
        "          \r\n",
        "    if(start!=-1 and end!=-1):\r\n",
        "        new_span.append([start,end])\r\n",
        "    answer=[]    \r\n",
        "    for v in new_span:\r\n",
        "      \r\n",
        "      answer.append([s[v[0]:v[1]+1],v[0],v[1]])\r\n",
        "    \r\n",
        "    \r\n",
        "    para.append(answer)\r\n",
        "  print(para)  \r\n",
        "  Qual.append(para)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[], [], [], [], []]\n",
            "[[], [], [], []]\n",
            "[[], [], [], [], [], [], [], [], []]\n",
            "[[], [], [], [], [['TH+', 101, 103], ['survived after', 115, 128]], [], [['The', 0, 2], ['the', 34, 36], ['the', 136, 138], ['after the transplant.', 164, 184]], [], [], [], [], [], [], []]\n",
            "[[], [], [], [], [], [], [], [], []]\n",
            "[[]]\n",
            "[[], []]\n",
            "[[], [], [['for', 39, 41], ['for HC', 84, 89], ['for SZ.', 102, 108]], [['for magnetometer', 39, 54], ['VESTAL program', 67, 80], ['for HC', 93, 98], ['for SZ.', 111, 117]], [['gradiometer', 66, 76], ['magnetometer', 111, 122]]]\n",
            "[[], [], []]\n",
            "[[['after transplantation in the testes', 37, 71], ['SCID', 76, 79]], []]\n",
            "[[], [], [], [], [], [], [], [], [], [], [], [['20 passages.', 94, 105]], [], [], [], []]\n",
            "[[], [], []]\n",
            "[[]]\n",
            "[[]]\n",
            "[[['plasma', 7, 12], ['weeks after transplantation).', 53, 81]]]\n",
            "[[], [], [], []]\n",
            "[[], [], [], [], [], [], [], [], []]\n",
            "[[['(No.', 59, 62], ['months after the transplant.', 83, 110]], [], [], []]\n",
            "[[]]\n",
            "[[], [], [], [], [], [], []]\n",
            "[[], [], [], [], [], []]\n",
            "[[], []]\n",
            "[[], [], [], []]\n",
            "[[['In', 0, 1], ['[11C]PK11195 in one allograft (animal', 59, 95]], [], [['the', 14, 16], ['IFN-γ temporarily increased at', 33, 62], ['months after the transplant in', 66, 95]], [], [], [], [], [], [], []]\n",
            "[[], [], [], [], [], []]\n",
            "[[], [], [], [], [], [], [], [], []]\n",
            "[[], [], [], [], [['weeks in culture.', 28, 44]]]\n",
            "[[], [], [], [], [], [], [], [], [], []]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IZu4_SNyBbW"
      },
      "source": [
        "for i in range(len(test_text)):\n",
        "  ans = []\n",
        "  offset = 0\n",
        "  k = 1\n",
        "  for j in range(len(test_text[i])):\n",
        "    # for span in Measured_Entity[i][j]:\n",
        "    #   ans.append([files[i][:-4],k, \"MeasuredEntity\", offset + span[1], offset + span[2]+1, \"T\" + str(k), test_text[i][j][span[1]:span[2] + 1], {}])\n",
        "    #   k+=1\n",
        "    for span in Measured_Quantity[i][j]:\n",
        "      ans.append([files[i][:-4],k, \"Quantity\", offset + span[1], offset + span[2]+1, \"T\" + str(k), test_text[i][j][span[1]:span[2] + 1], {}])\n",
        "      k+=1\n",
        "    # for span in Measured_Property[i][j]:\n",
        "    #   ans.append([files[i][:-4],k, \"MeasuredProperty\", offset + span[1], offset + span[2]+1, \"T\" + str(k), test_text[i][j][span[1]:span[2] + 1], {}])\n",
        "    #   k+=1\n",
        "    # for span in Qual[i][j]:\n",
        "    #   ans.append([files[i][:-4],k, \"Qualifier\", offset + span[1], offset + span[2]+1, \"T\" + str(k), test_text[i][j][span[1]:span[2] + 1], {}])\n",
        "    #   k+=1\n",
        "    offset += len(test_text[i][j])+1\n",
        "  df = pd.DataFrame(ans, columns = [\"docId\",\"annotSet\",\"annotType\", \"startOffset\",\t\"endOffset\", \"annotId\", \"text\", \"other\"]) \n",
        "  df.to_csv(\"/content/drive/MyDrive/Score_Same/\" + files[i][:-4] + \".tsv\", sep=\"\\t\",index=False)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVlhYcTyy077"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zZ0gE0g13mX"
      },
      "source": [
        "for i in range(len(test_text)):\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvLOSKCp01Oh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "856zb7R4HreM"
      },
      "source": [
        "Extra Information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGUaHKM6HopM"
      },
      "source": [
        "class BERT_Arch(nn.Module):\r\n",
        "\r\n",
        "    def __init__(self, bert, embed_dim, hidden_dim, drop_prob, n_layers, out_dim):\r\n",
        "      \r\n",
        "      super(BERT_Arch, self).__init__()\r\n",
        "\r\n",
        "      self.bert = bert \r\n",
        "      self.n_layers = n_layers\r\n",
        "      self.bilstm = nn.LSTM(embed_dim, hidden_dim,  bidirectional=True, batch_first=True)\r\n",
        "      self.dropout = nn.Dropout(drop_prob)\r\n",
        "      self.fc1 = nn.Linear(2*hidden_dim,out_dim)\r\n",
        "      self.sigmoid = nn.Sigmoid()\r\n",
        "\r\n",
        "    #define the forward pass\r\n",
        "    def forward(self, sent_id, mask):\r\n",
        "\r\n",
        "      \r\n",
        "      # embed,cls_hs = self.bert(sent_id, attention_mask=mask)\r\n",
        "      \r\n",
        "      x = self.bert(sent_id, attention_mask=mask)\r\n",
        "      embed=x[0]\r\n",
        "      cls_hs=x[1]\r\n",
        "      \r\n",
        "      x,_ = self.bilstm(embed)\r\n",
        "      \r\n",
        "      x = self.dropout(x)\r\n",
        "      x = self.fc1(x)\r\n",
        "      x = self.sigmoid(x)\r\n",
        "\r\n",
        "      return x\r\n",
        "\r\n",
        "bert_model = BERT_Arch(model, 768, 256, 0.5, 1,1)\r\n",
        "bert_model = bert_model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfhrWctIfyaV"
      },
      "source": [
        "idx2labels = {0:'HasTolerance',\r\n",
        " 1:'IsApproximate',\r\n",
        " 2:'IsCount',\r\n",
        " 3:'IsList',\r\n",
        " 4:'IsMean',\r\n",
        " 5:'IsMeanHasSD',\r\n",
        " 6:'IsMeanHasTolerance',\r\n",
        " 7:'IsMeanIsRange',\r\n",
        " 8:'IsMedian',\r\n",
        " 9:'IsRange',\r\n",
        " 10:'IsRangeHasTolerance'}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3BFoxi9HGv5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "c177acdc-aaac-4efc-d367-6409ec5712ca"
      },
      "source": [
        "bert_model = torch.load(INFORMATION_DETECTION_PATH)\r\n",
        "info=[]\r\n",
        "for paragraph in test_text:\r\n",
        "  for s in paragraph:\r\n",
        "    tokenized_text = tokenizer.tokenize(s)\r\n",
        "    test_inputs = tokenizer(s,max_length = 512,padding='max_length',truncation=True, return_tensors=\"pt\")\r\n",
        "    bert_model.eval()\r\n",
        "    temp = bert_model(test_inputs['input_ids'].to(device), test_inputs['attention_mask'].to(device))>=0.3\r\n",
        "    \r\n",
        "    b = {}\r\n",
        "    for m in re.finditer(r'[.,\\S]+', s):\r\n",
        "      w = m.group(0)\r\n",
        "      t = (tokenizer.encode(w, add_special_tokens=False), (m.start(), m.end()-1))\r\n",
        "      vec=t[0]\r\n",
        "      for j in vec:\r\n",
        "        if j in b:\r\n",
        "          b[j].append(t[1])\r\n",
        "        else:\r\n",
        "\r\n",
        "          b[j]=[t[1]]\r\n",
        "    \r\n",
        "    \r\n",
        "    quant={}\r\n",
        "    \r\n",
        "    \r\n",
        "    txt=[]\r\n",
        "    for i in range(len(temp[0])):\r\n",
        "      for k in range(len(temp[0][i-1][k])):\r\n",
        "        if(temp[0][i]):\r\n",
        "          txt.append(tokenized_text[i-1])\r\n",
        "          if(id[i] in b):\r\n",
        "            vec=b[id[i]]\r\n",
        "        \r\n",
        "            for q in vec:\r\n",
        "              quant[q]=1\r\n",
        "    l=[]\r\n",
        "    span=np.zeros(len(s))\r\n",
        "    for key,val in quant.items():\r\n",
        "      start=key[0]\r\n",
        "      end=key[1]\r\n",
        "      span[start:end+1]=1\r\n",
        "    new_span=[]\r\n",
        "    \r\n",
        "    start=-1\r\n",
        "    end=-1\r\n",
        "    for i in range(len(s)):\r\n",
        "      if(span[i]==1):\r\n",
        "        if(start==-1):\r\n",
        "          start=i\r\n",
        "          end=i\r\n",
        "        else:\r\n",
        "          end=i\r\n",
        "      else:\r\n",
        "        if(start==-1):\r\n",
        "          end=-1\r\n",
        "          continue\r\n",
        "        elif(span[i-1]==1 and span[i+1]==1):\r\n",
        "          end=i;\r\n",
        "        else:\r\n",
        "          new_span.append([start,end])\r\n",
        "          start=-1\r\n",
        "          end=-1\r\n",
        "\r\n",
        "          \r\n",
        "    if(start!=-1 and end!=-1):\r\n",
        "        new_span.append([start,end])\r\n",
        "    answer=[]    \r\n",
        "    for v in new_span:\r\n",
        "      \r\n",
        "      answer.append([s[v[0]:v[1]+1],v[0],v[1]])\r\n",
        "    print(s)\r\n",
        "    print(txt)\r\n",
        "    print(answer)\r\n",
        "    \r\n",
        "    \r\n",
        "    \r\n",
        "    \r\n",
        "    info.append(answer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-68-564b9892feaa>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    b = {}\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91D5MrIjImF1"
      },
      "source": [
        "idx2labels = {0:'HasTolerance',\r\n",
        " 1:'IsApproximate',\r\n",
        " 2:'IsCount',\r\n",
        " 3:'IsList',\r\n",
        " 4:'IsMean',\r\n",
        " 5:'IsMeanHasSD',\r\n",
        " 6:'IsMeanHasTolerance',\r\n",
        " 7:'IsMeanIsRange',\r\n",
        " 8:'IsMedian',\r\n",
        " 9:'IsRange',\r\n",
        " 10:'IsRangeHasTolerance'}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKHEc2g8II-t",
        "outputId": "b9ecdb87-b584-49bc-96fb-86f9aa54ac7e"
      },
      "source": [
        "for i in range(len(test_text)):\r\n",
        "  tokenized_text = tokenizer.tokenize(test_text[i])\r\n",
        "  txt=[]\r\n",
        "  print(test_text[i])\r\n",
        "  \r\n",
        "  for j in range(len(tokenized_text)):\r\n",
        "    for k in range(11):\r\n",
        "      if(info[i][j+1][k]):\r\n",
        "        txt.append(tokenized_text[j]+\" \"+idx2labels[k])   \r\n",
        "  print(txt)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(F) Adult hRPE cultures display expression of markers typical of native RPE in their polarized localization.\n",
            "[]\n",
            "DAPI is cyan, whereas all other immunofluorescence is gold.\n",
            "[]\n",
            "Claudin 19, ezrin, ZO1, and MCT1 are preferentially located on the apical side.\n",
            "[]\n",
            "RPE65 and CRALBP are cytoplasmic.\n",
            "[]\n",
            "The scale bars represent 10 μm.\n",
            "[]\n",
            "GFP-expressing H9, BBHX8, and A1ATD-1 cells were generated by stable transfection using lipofectamine 2000 (Invitrogen) as described previously (Vallier et al., 2001).\n",
            "[]\n",
            "GFP-positive cells were differentiated into foregut cells and then dissociated into single cells.\n",
            "[]\n",
            "An individually isolated GFP cell was then transferred into a well containing non-GFP-positive hFSCs.\n",
            "[]\n",
            "Wells were visually inspected 12 hr after plating, and wells containing a single GFP-positive hFSC were selected for clonal expansion.\n",
            "['hr IsRange']\n",
            "For group analyses, the following procedures were applied.\n",
            "[]\n",
            "(1) T1-weighted sMRIs from each subject (Fig.\n",
            "[]\n",
            "1A) were registered to MNI space (Montreal Neurological Institute, MNI-152 atlas as in Fig.\n",
            "[]\n",
            "1B) using an affine transformation (FLIRT–FMRIB's Linear Image Registration Tool) (Jenkinson and Smith, 2001) in FSL (www.fmrib.ox.ac.uk/fsl/).\n",
            "[]\n",
            "(2) The cortical (Fig.\n",
            "[]\n",
            "1C) and subcortical masks with pre-defined brain regions from the standard atlas were transferred to the individual's headspace (Fig.\n",
            "[]\n",
            "1D), using the inverse of the transformation obtained in the first step: the Harvard-Oxford Atlas, part of the FSL software with masks of 96 cortical gray-matter regions (48 regions in each hemisphere), 21 sub-cortical regions, and cerebellum, was used.\n",
            "[]\n",
            "(3) The regional masks were down-sampled to a cubic source grid with voxels of 5 mm per side (Fig.\n",
            "[]\n",
            "1E).\n",
            "[]\n",
            "(4) VESTAL MEG source imaging used the source grid from step 3.\n",
            "[]\n",
            "This step permits group-based analyses.\n",
            "[]\n",
            "In the shown example, MEG responses evoked by S1 localized to left and right Heschl's gyri (Fig.\n",
            "[]\n",
            "1F).\n",
            "[]\n",
            "(5) Finally, for regions of interest (ROIs), the source time course was obtained by summing activity from all ROI voxels.\n",
            "[]\n",
            "Fig.\n",
            "[]\n",
            "1H shows the time course from left Heschl's gyrus (dark blue region in Fig.\n",
            "[]\n",
            "1C and D).\n",
            "[]\n",
            "Another important finding is that, in spite of the immune responses mounted by the host brain, a substantial number of TH+ cells survived in the allografts.\n",
            "[]\n",
            "This is consistent with previous clinical reports of human fetal cell transplantation.\n",
            "[]\n",
            "Postmortem analyses of the patients revealed robust survival of DA neurons in spite of the fact that numerous immune cells were present around the graft (Kordower et al., 1997).\n",
            "[]\n",
            "In two double-blind clinical trials, immunosuppressive drugs were never used (Freed et al., 2001) or were withdrawn after 6 months (Olanow et al., 2003).\n",
            "['two IsCount', '6 IsRange', 'months IsRange']\n",
            "In these cases, the cells from multiple fetuses were used without HLA matching, but more than 50,000 TH+ cells had survived after several years.\n",
            "['than IsRange', '50 IsRange', ', IsRange', '000 IsRange', 'th IsRange']\n",
            "Our quantitative PCR (qPCR) study in vitro showed that the expression of MHC-I increased in response to IFN-γ, but the expression level was still 1/10 that of untreated monkey peripheral blood cells (Figure 2B).\n",
            "['/ IsApproximate', '10 IsApproximate']\n",
            "The in vivo studies revealed that the serum level of IFN-γ increased at 2 months, and CD45+ cells (including CD8+ cells) accumulated in the allografts 3.5–4 months after the transplant.\n",
            "['2 IsRange', '3 IsRange', '. IsRange', '5 IsRange', '– IsRange', '4 IsRange', 'months IsRange']\n",
            "On the other hand, the levels of INF-γ in the cerebrospinal fluid (CSF) and the levels of tumor necrosis factor α (TNF-α) in both the serum and CSF were below the limit of detection by ELISA (data not shown).\n",
            "[]\n",
            "An immunofluorescence study did not reveal any apparent expression of MHC-I by the grafted cells (Figure S4A).\n",
            "[]\n",
            "Therefore, it is possible that the immune response in the primate brain was not strong enough to reject all of the donor cells.\n",
            "[]\n",
            "These findings closely correlate with the results of previous murine experiments (Hudson et al., 1994; Shinoda et al., 1995).\n",
            "[]\n",
            "To apply our findings to a more clinically relevant setting, we investigated the expression of HLA-I during neural differentiation of human ESCs (hESCs) and iPSCs by qPCR (Figure S4D).\n",
            "[]\n",
            "The expression level was 1/100 compared with that of human peripheral blood cells in both hESCs and iPSCs, and it was similarly elevated in response to IFN-γ.\n",
            "[]\n",
            "It is difficult to precisely compare immunogenicity in monkeys with that in humans, but the low expression level of MHC-I by the donor cells may account for the mild rejection in both monkey and human neural transplantation.\n",
            "[]\n",
            "We next addressed whether hESC-derived trigeminal neurons can engraft in the adult mouse CNS and project toward their physiological target.\n",
            "[]\n",
            "The trigeminal nuclei in the brainstem receive afferent innervation from the trigeminal sensory ganglion that is relayed to the contralateral thalamus.\n",
            "[]\n",
            "The pons was selected as site for transplantation, because it is surgically accessible and located within proximity of the trigeminal brain stem nuclei that receive afferent input from the trigeminal ganglia.\n",
            "[]\n",
            "Hence, GFP+ human trigeminal neuron clusters were injected into adult NOD/SCID mice via stereotactic surgery (see Experimental Procedures).\n",
            "[]\n",
            "Histological analysis 4 weeks after transplantation showed survival of GFP+ human cell graft in the ventral pons (Figure S5E).\n",
            "['weeks IsRange']\n",
            "Although GFP+ cell bodies remained tightly clustered at injection site, GFP+ fibers showed extensive projections into the host brain (n = 6) including the endogenous trigeminal nuclei (Figure S5F).\n",
            "[]\n",
            "Expression of BRN3A confirmed the sensory neuron identity of the cells (Figure S5G).\n",
            "[]\n",
            "Graft-derived human fiber bundles (hNCAM+ and GFP+) were observed emanating from the graft core (Figure S5H).\n",
            "[]\n",
            "These data demonstrate in vivo survival of trigeminal placode derivatives, differentiation along sensory neuron lineage, and the establishment of axonal projections toward relevant endogenous targets in the embryonic chick and adult mouse brain.\n",
            "[]\n",
            "(B) Fetal hRPE stained for SC121 (red) and MCT1 (green; scale bars, 125 μm and 25 μm [inset]).\n",
            "['##m IsRange']\n",
            "(F) Intracellular PGRN levels in iPSC-derived neurons after medium collection.\n",
            "[]\n",
            "Values of control line 20 were set to 100% (n = 3–4 independent cultures).\n",
            "[]\n",
            "Prior to VESTAL analyses, a 5–55 Hz bandpass filter was applied.\n",
            "[]\n",
            "VESTAL analyses examined activity 30–130 ms post-stimulus producing a 4D activation map (3D volumes across time) as well as a 2D source time-course matrix.\n",
            "['30 IsRange', '– IsRange', '130 IsRange', 'ms IsRange']\n",
            "The average percent variance explained for gradiometer data using VESTAL was 95.81% for HC and 94.38% for SZ.\n",
            "[]\n",
            "The average percent variance explained for magnetometer data using VESTAL program was 96.24% for HC and 93.17% for SZ.\n",
            "[]\n",
            "There were no group differences in percent variance explained for gradiometer data (t(39) = 1.16, p = 0.25) or magnetometer data (t(39) = 1.31, p = 0.20).\n",
            "[]\n",
            "(C) Control lines 16 and 17, sporadic lines 12 and 23, and PGRN S116X lines 1 and 14 were immunostained for AFP (endoderm), desmin (mesoderm), and βIII-tubulin (ectoderm), and counterstained with DAPI (nuclei).\n",
            "[]\n",
            "All lines showed a normal karyotype.\n",
            "[]\n",
            "Scale bar: 50 μm.\n",
            "[]\n",
            "(I–M) Teratoma formation at 3 months after transplantation in the testes of SCID mice.\n",
            "['3 IsRange', 'months IsRange']\n",
            "H&E staining of the sections showed histological features of the neuroepithelium (J), cartilage (K), muscle (L) and gut-like epithelium (M).\n",
            "[]\n",
            "Using a recombinant adeno-associated viral (rAAV)-based gene-targeting method, we inserted the gene-encoding GFP into the SOX2 locus in H9 hESCs (Figure 1A).\n",
            "[]\n",
            "Proper homologous recombination led to the replacement of the SOX2 open reading frame with that of GFP and a neomycin selection cassette (SV40-Neo).\n",
            "[]\n",
            "After infection with rAAV and G418 drug selection, a total of 36 clones were expanded and screened by Southern blotting for homologous recombination events.\n",
            "['36 IsCount']\n",
            "Among these clones, 26 (72%) were found to carry the GFP-Neo cassette in the SOX2 locus (Figure S1A available online).\n",
            "[]\n",
            "No clones in which both SOX2 alleles were disrupted were isolated.\n",
            "[]\n",
            "Our subsequent analysis focused on one of these clones, clone 23 (hSOX2-23).\n",
            "[]\n",
            "We confirmed appropriate gene targeting in this clone using multiple restriction digests followed by Southern blotting (Figures 1B, S1B, and S1C).\n",
            "[]\n",
            "We did not observe nontargeted insertions of the rAAV sequences, and cells exhibited a normal karyotype (data not shown).\n",
            "[]\n",
            "Flow cytometry of hSOX2-23 revealed that the majority of the cells expressed GFP (Figure 1C).\n",
            "[]\n",
            "By comparison, a drug-selected clone, hSOX2-25, which was negative for targeted insertion (Figure S1A), showed no detectable GFP (Figure S2A).\n",
            "[]\n",
            "Despite only having one copy of SOX2, hSOX2-23 had similar levels of SOX2, OCT4, and NANOG expression as hSOX2-25 and wild-type (WT) hESCs (Figure S2B).\n",
            "['one IsCount']\n",
            "Moreover, the percentage of GFP-positive (GFP+) cells in hSOX2-23 was constant over more than 20 passages.\n",
            "['more IsRange', 'than IsRange', '20 IsRange', 'passages IsRange', '. IsRange']\n",
            "Immunofluorescence (IF) staining of hSOX2-23 showed that 100% of GFP+ cells expressed SOX2 protein (Figure S2C).\n",
            "[]\n",
            "Additionally, hSOX2-23 colonies had characteristic hESC morphology (Figure S2D) and expressed markers of the undifferentiated state, such as NANOG (Figure S2E).\n",
            "[]\n",
            "These results show that this rAAV-based gene-targeting method can be used to efficiently disrupt genes by homologous recombination.\n",
            "[]\n",
            "In addition, the SOX2-GFP hESC marker line can be used to monitor SOX2 expression in undifferentiated hESCs.\n",
            "[]\n",
            "Based on several previous publications (Khan et al., 2010, 2011), we explored the utility of adeno-associated virus (AAV) as a method to improve gene targeting efficiencies in hPSCs.\n",
            "[]\n",
            "For SOX2, a gene that is highly expressed in undifferentiated hPSCs, gene targeting rates were greater than 70%.\n",
            "['than IsRange', '70 IsRange', '% IsRange']\n",
            "Similar targeting efficiencies in hPSCs using AAV have been reported by others (Asuri et al., 2012; Khan et al., 2010, 2011; Smith-Arica et al., 2003), indicating that AAV offers a highly efficient and robust approach to target genes for HR in hPSCs.\n",
            "[]\n",
            "(C) C-peptide and PDX1 expression was confirmed by immunocytochemistry of cells differentiated for 25 days.\n",
            "['25 IsRange', 'days IsRange']\n",
            "•Adult hRPESC-derived RPE had comparable in vitro characteristics to fetal hRPE•hRPE monolayers survived 4 weeks on PET carriers under the rabbit retina•Better xenograft survival may be due to the maintained hRPE cell polarity•Atrophy of the retina overlaying the hRPE xenograft remains a future challenge\n",
            "[]\n",
            "(O) GH plasma levels using a human specific ELISA (6 weeks after transplantation).\n",
            "[]\n",
            "(R) Quantification of immunocytochemical analyses for each iPSC line.\n",
            "[]\n",
            "Data are shown as the means ± SD (n = 3 independent experiments).\n",
            "[]\n",
            "SER, serotonin, TUBβIII, β-tubulin class III.\n",
            "[]\n",
            "Scale bars: 200 μm in (A)–(H), 50 μm in insets of (C)–(F), 100 μm in (J)–(M) and (Q).\n",
            "[]\n",
            "The two FTD patients under investigation in this study were part of a longitudinal dementia research program at the Memory and Aging Center, University of California, San Francisco.\n",
            "['two IsCount']\n",
            "Both had an 8-year history of behavioral changes and memory impairment at the time of tissue collection for this study.\n",
            "[]\n",
            "One patient, a 67-year-old male with sporadic FTD, tested negative for mutations in GRN, MAPT, and C9ORF72.\n",
            "[]\n",
            "The other patient, a 64-year-old male with a significant family history of dementia, had behavioral variant FTD.\n",
            "[]\n",
            "MRI in this patient demonstrated severe bifrontal and temporal atrophy associated with gliosis in the frontal lobes (greater on the right).\n",
            "[]\n",
            "One year later, MRI scans showed progression of atrophy and gliosis.\n",
            "[]\n",
            "Genetic testing revealed a novel nonsense mutation in GRN, p.S116X (g.4627C > A, c.347C > A), which is predicted to result in a premature stop codon.\n",
            "[]\n",
            "Both FTD patients had parkinsonism, which is typical of all FTD patients with PGRN mutations.\n",
            "[]\n",
            "An age-matched subject, a clinically normal 64-year-old male with no mutations in GRN, MAPT, or C9ORF72, served as a control.\n",
            "[]\n",
            "(Q–S) Magnetic resonance images of a representative animal (No.\n",
            "[]\n",
            "6, autograft) at 3 months after the transplant.\n",
            "['3 IsRange', 'months IsRange']\n",
            "The arrowheads indicate the directions of the cell injections.\n",
            "[]\n",
            "(Q) coronal, (R) axial, and (S) sagittal.\n",
            "[]\n",
            "The letter L indicates the left side.\n",
            "[]\n",
            "(A) Fetal hRPE stained for pan-cytokeratin (scale bar, 50 μm); inset shows section overview stained with hematoxylin/eosin (scale bar, 200 μm).\n",
            "[]\n",
            "To assess the in vivo properties of hESC-derived trigeminal placode precursors, PIP-induced neuronal clusters, derived from a constitutively GFP-positive hESC line (Figures S5A and S5B), were injected into the developing chick embryo targeting the early trigeminal anlage at H&H stage 10–12 (Figure S5C).\n",
            "[]\n",
            "Human cells were identified based on GFP expression and use of human specific antibodies against cytoplasmic antigen (hCA).\n",
            "[]\n",
            "Two days after in ovo transplantation, surviving GFP+ cells were found dispersed in the area of the endogenous chick trigeminal ganglion (Figure 4P).\n",
            "[]\n",
            "We observed extensive GFP+ human fiber bundles coexpressing hCA and peripherin (Figures 4Q and 4R).\n",
            "[]\n",
            "In contrast, no hCA or peripherin expression was detected in the neural tube of the embryo (Figure S5D).\n",
            "[]\n",
            "The in vivo fiber outgrowth 2 days after transplantation was reminiscent of the extensive in vitro fiber outgrowth of replated trigeminal neuron clusters (Figure S5A).\n",
            "['days IsRange']\n",
            "Peripherin expression in vivo (Figure 4S) confirmed the peripheral neuron identity of the grafted cells.\n",
            "[]\n",
            "For the first two animals (Nos.\n",
            "['first IsCount', 'two IsCount']\n",
            "1 and 4), we established iPSCs from fibroblasts derived from the oral mucosa using retroviral vectors (Okita et al., 2011).\n",
            "[]\n",
            "For the other two animals (Nos.\n",
            "['two IsCount']\n",
            "6 and 8), we used peripheral blood mononuclear cells (PBMCs) with nonintegrating episomal vectors (Okita et al., 2013).\n",
            "[]\n",
            "We selected the best clone from each animal according to the following criteria: a stable embryonic stem cell (ESC)-like morphology of the colonies after passaging, expression of pluripotent markers, few or no integrated transgenes (Figures 1A–1F; Figure S1 available online), and the potential for stable neural differentiation.\n",
            "['following IsCount']\n",
            "A PCR analysis revealed that all of the clones with retroviral vectors showed apparent expression of remaining transgenes (Figures S1C and S1D), whereas the clones with episomal vectors never did (Figure S1F).\n",
            "['all IsCount']\n",
            "To detect the iPSC-derived cells in a brain, we introduced GFP (Figures 1G and 1H).\n",
            "[]\n",
            "The selected clones of iPSCs had the potential to generate teratomas in the testes of a severe combined immunodeficiency (SCID) mouse within 12 weeks (Figures 1I–1M).\n",
            "['12 IsRange']\n",
            "(A) Flow-cytometric analyses for MHC-I (HLA-A, HLA-B, and HLA-C).\n",
            "[]\n",
            "Incubation of the cells with IFN-γ for 48 hr increased the MHC-I expression (green).\n",
            "[]\n",
            "Our group has developed a defined culture system to direct the differentiation of hPSCs into a near-homogenous population of definitive endoderm (DE) cells that have the capacity to differentiate into hepatocytes and pancreatic progenitors (Brown et al., 2011; Cho et al., 2012; Rashid et al., 2010; Touboul et al., 2010; Vallier et al., 2009a; Yusa et al., 2011).\n",
            "[]\n",
            "Cells grown in these culture conditions successively express primitive streak markers (T and Mixl1), downregulate pluripotency markers (NANOG, SOX2, and POU5F1) and progressively upregulate definitive endoderm markers (CXCR4, FOXA2, GATA4, CERB, and SOX17) (Figures S1A–S1C available online).\n",
            "[]\n",
            "Flow cytometry analyses showed that 80% of the resulting DE population coexpresses CXCR4 and SOX17 (Figure S1D).\n",
            "['% IsApproximate']\n",
            "Interestingly, the resulting population of DE cells is negative for genes marking the foregut (SOX2), the midgut/hindgut (CDX2), the pancreas (PDX1), the liver (AFP), and the lungs (HOXA1) (Figures S1E and S1F) This confirms that DE cells generated in vitro could correspond to early endoderm progenitor cells prior to anteroposterior patterning or organogenesis.\n",
            "[]\n",
            "In sequential PET studies, we observed increased uptake of [11C]PK11195 in one allograft (animal No.\n",
            "[]\n",
            "10) at 3 months (Figures 3A and 3B).\n",
            "[]\n",
            "We could not detect any apparent uptake in the other animals or at any other time points (Figure S2).\n",
            "[]\n",
            "Intriguingly, the serum level of IFN-γ temporarily increased at 2 months after the transplant in three animals (Figure 3C).\n",
            "['2 IsRange', 'months IsRange', 'three IsCount']\n",
            "An immunofluorescence study conducted at 3.5–4 months showed that MHC-II+ cells were more frequently found in allografts than in autografts, especially in the monkey with increased uptake of [11C]PK11195 (Figure 3D, No.\n",
            "['3 IsRange', '. IsRange', '5 IsRange', '– IsRange', '4 IsRange', 'months IsRange']\n",
            "10).\n",
            "[]\n",
            "The MHC-II staining never overlapped with that of GFP of the donor cells (Figure 3F), whereas it generally overlapped with that of IBA1 (Figure 3G), indicating that MHC-II was expressed by host-derived microglia.\n",
            "[]\n",
            "Consistently, the number and density of IBA1+ cells were higher in allografts than in autografts (Figures 3E, 3H, and S4C).\n",
            "[]\n",
            "An increase in the expression of MHC might trigger the recruitment of circulating immune cells, including T cells.\n",
            "[]\n",
            "An immunofluorescence study revealed that more CD45+ cells (a marker for pan-leukocytes) accumulated in allografts compared with autografts (Figures 3I and 3J).\n",
            "[]\n",
            "Most of the CD45+ cells were CD3+ T cells, and 60% of them were CD8+ killer T cells (Figures 3K and 3L).\n",
            "[]\n",
            "These findings suggest that an acquired immune response was elicited only in the allografts in the primate brain.\n",
            "[]\n",
            "This study was approved by the Institutional Review Board and Ethics Committees of the University of California, San Francisco, and written informed consent was obtained in all cases.\n",
            "[]\n",
            "The patient with the PGRN S116X mutation followed the classic clinical progression for FTD and developed parkinsonism, as do all FTD patients with PGRN mutations, but he did not show typical features of PD dementia.\n",
            "[]\n",
            "The patient with sporadic FTD also showed parkinsonism.\n",
            "[]\n",
            "Skin biopsies were collected, cut into small pieces, and placed on culture dishes to allow the fibroblasts to expand.\n",
            "[]\n",
            "The cells were maintained in Dulbecco's modified Eagle's medium supplemented with 10% fetal bovine serum, 1X nonessential amino acids, and penicillin/streptomycin (100 U/ml).\n",
            "[]\n",
            "iPSCs were generated as described previously (Takahashi et al., 2007).\n",
            "[]\n",
            "Please see Supplemental Information for more details.\n",
            "[]\n",
            "(D–H) Adult hRPE stained for SC121 (red).\n",
            "[]\n",
            "(D) Adult hRPE stained for MCT1 (green).\n",
            "[]\n",
            "(E) Human adult RPE stained for ezrin (green).\n",
            "[]\n",
            "hRPEs transplanted into rabbit SRS show absence of expression of ki67 (F), phosphohistone H3 (G), and caspase-3 (H).\n",
            "[]\n",
            "Polarized fetal and adult hRPE cells were found in TEM (I and J).\n",
            "[]\n",
            "Nuclei with regular chromatin were found in the basal compartment, a basal lamina ([I], large black arrowhead) had formed between the xenograft and PET carrier (black asterisks).\n",
            "[]\n",
            "Melanosomes (M) in multiple stages, some microvilli abutting to the atrophic neural retina (NR), and junctional structures with desmosomes (small black arrowhead) and tight junctions (red arrowhead) were discerned apically.\n",
            "[]\n",
            "Mitochondria (MC) were seen in the basolateral part of the cell.\n",
            "[]\n",
            "Detachment from cell carrier (asterisk) in (J) is a histologic processing artifact.\n",
            "[]\n",
            "Left images in (I) and (J) taken at 10,500× magnification; right micrographs are rectangular zone in left at 25,000×; scale bars represent 2 μm/inset 0.2 μm distance in (I) and (J).\n",
            "[', IsList', '500 IsList', '000 IsList']\n",
            "iPSC colonies were detached with accutase (Millipore) and grown as embryoid bodies (EBs) in suspension for 5–6 days in iPSC medium without basic fibroblast growth factor.\n",
            "['– IsRange', '6 IsRange']\n",
            "EBs were allowed to attach and form rosettes.\n",
            "[]\n",
            "Ten-day-old rosettes were collected and grown in suspension as neurospheres.\n",
            "[]\n",
            "Neurospheres were dissociated after 3–4 weeks, and the cells were plated on glass coverslips (BD Biosciences) or plates coated with poly-D-lysine (0.1 mg/ml) and laminin (10 μg/m).\n",
            "['3 IsRange', '– IsRange', '4 IsRange', 'weeks IsRange', ', IsRange']\n",
            "Neurons were used after 2-4 weeks in culture.\n",
            "['2 IsRange', '- IsRange', '4 IsRange', 'weeks IsRange']\n",
            "For cell differentiation, FAP-specific iPS cells were cultured with serial changes of media as shown in Fig.\n",
            "[]\n",
            "3A.\n",
            "[]\n",
            "To test whether FAP-specific iPS cells can differentiate into hepatocyte-like cells, we analyzed several markers via real-time PCR analysis on Day 5 (D5), D13 and D20 differentiated FAP-specific iPS cells (Fig.\n",
            "[]\n",
            "3B).\n",
            "[]\n",
            "A decrease in expression of the pluripotency marker Oct3/4 was accompanied by differentiation of FAP-specific iPS cells.\n",
            "[]\n",
            "Expression of the endoderm marker Sox17 was observed on D5 differentiation and decreased gradually after the medium was changed to hepatic differentiation medium on D7 (Fig.\n",
            "[]\n",
            "3B).\n",
            "[]\n",
            "The hepatic progenitor marker AFP and the mature hepatocyte marker ALB were obviously expressed on D13 and D20.\n",
            "[]\n",
            "In addition, immunocytochemical analyses showed Sox17 expression on D5, both HNF-4α and AFP expression on D13, and ALB cytoplasmic staining on D20 (Fig.\n",
            "[]\n",
            "3C).\n",
            "[]\n",
            "Quantitative imaging analysis revealed that approximately 78 ± 0.6% of cells were Sox17-positive on D5 and approximately 88 ± 1.1% of cells were AFP-positive on D13 and approximately 29 ± 0.9% of cells were ALB-positive on D20 (Fig.\n",
            "['approximately IsApproximate', '78 HasTolerance', '± HasTolerance', '0 HasTolerance', '. HasTolerance', '6 HasTolerance', '% HasTolerance', '88 HasTolerance', '± HasTolerance', '± HasTolerance']\n",
            "3D).\n",
            "[]\n",
            "The ALB secretion in the media of differentiated FAP-specific iPS cells on D20 was approximately 20 μg/ml (Fig.\n",
            "['approximately IsApproximate', '20 IsApproximate', 'μ IsApproximate', '##g IsApproximate', '/ IsApproximate', 'ml IsApproximate']\n",
            "3E).\n",
            "[]\n",
            "Moreover, these D20 differentiated FAP-specific iPS cells were also periodic acid-Schiff (PAS)-positive, indicating cytoplasmic glycogen storage (Fig.\n",
            "[]\n",
            "3F).\n",
            "[]\n",
            "These results clearly indicated that FAP-specific iPS cells had the potential to differentiate into hepatocyte-like cells.\n",
            "[]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3onHFGI7JGNc"
      },
      "source": [
        "Unit Detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfTglERDIO9R"
      },
      "source": [
        "file = open(VOCAB_PATH,'rb')\r\n",
        "TEXT = pickle.load(file)\r\n",
        "vocab_size=len(TEXT.vocab)\r\n",
        "class bilstm(nn.Module):\r\n",
        "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers):\r\n",
        "        super().__init__()\r\n",
        "        self.hidden_dim = hidden_dim\r\n",
        "        self.n_layers = n_layers\r\n",
        "        self.output_size = output_size\r\n",
        "        \r\n",
        "        self.embedding = nn.Embedding(vocab_size,embedding_dim)\r\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \r\n",
        "                         batch_first=True, bidirectional=True)\r\n",
        "        \r\n",
        "        self.dropout = nn.Dropout(0)\r\n",
        "        self.fc = nn.Linear(hidden_dim*2,output_size)\r\n",
        "        self.sigmoid = nn.Sigmoid()\r\n",
        "        \r\n",
        "    def forward(self,x):\r\n",
        "        embeds = self.embedding(x)\r\n",
        "        lstm, (hn,cn)= self.lstm(embeds)\r\n",
        "        \r\n",
        "        output = self.dropout(lstm)\r\n",
        "        output = self.fc(output)\r\n",
        "        output = self.sigmoid(output) \r\n",
        "        return output\r\n",
        "device = \"cpu\"\r\n",
        "model = bilstm(vocab_size, 1, 100, 32, 1)\r\n",
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2bnUdAYJLzB"
      },
      "source": [
        "model = torch.load(UNIT_DETECTION_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8hvkoWfJRPv",
        "outputId": "ab3fedcb-2bd7-4d6c-b01b-8afba2cfea0f"
      },
      "source": [
        "\r\n",
        "\r\n",
        "for i in range(len(Measured_Quantity)):\r\n",
        "  for j in range(len(Measured_Quantity[i])):  \r\n",
        "      quantity=Measured_Quantity[i][j]\r\n",
        "      X_train.append(TEXT.process(quantity)[0].numpy())\r\n",
        "      \r\n",
        "      X_train=np.array(X_train)\r\n",
        "\r\n",
        "      \r\n",
        "      X_train=torch.from_numpy(X_train)\r\n",
        "      X_train=X_train[:,:,0]\r\n",
        "\r\n",
        "\r\n",
        "      \r\n",
        "      row=X_train.shape[1]\r\n",
        "      \r\n",
        "      \r\n",
        "      X_train=F.pad(input=X_train, pad=(0, 64-row, 0, 0), mode='constant', value=1)\r\n",
        "      \r\n",
        "      model.zero_grad()\r\n",
        "      model.eval()\r\n",
        "      y_pred = model(X_train.to(device))\r\n",
        "      \r\n",
        "      np_out = y_pred.cpu().data.numpy()\r\n",
        "      \r\n",
        "      np_out=np_out>=0.75\r\n",
        "      un=\"\"\r\n",
        "      for k in range(len(np_out[0])):\r\n",
        "        if(np_out[0][k]):\r\n",
        "          un=un+j[k]\r\n",
        "      if(len(un)==0):\r\n",
        "        print(\"Quantity-\"+j,\"Unit-None\")\r\n",
        "      else:\r\n",
        "        print(\"Quantity-\"+j,\"Unit-\"+un)\r\n",
        "      unit.append(un)\r\n",
        "\r\n",
        "    UNITS.append(unit)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[['10 μm.', 25, 30]]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[['12 hr', 30, 34]]\n",
            "[]\n",
            "[]\n",
            "[['96', 295, 296], ['21', 360, 361]]\n",
            "[['5 mm per side', 79, 91]]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[['two', 3, 5], ['after 6 months', 116, 129]]\n",
            "[['cases,', 9, 14], ['matching,', 70, 78], ['more than 50,000', 84, 99]]\n",
            "[['that', 50, 53], ['1/10 that', 146, 154]]\n",
            "[['2 months,', 72, 80], ['3.5–4 months', 151, 162], ['transplant.', 174, 184]]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[['1/100', 25, 29]]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[['4 weeks', 22, 28]]\n",
            "[['6)', 138, 139]]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[['125 μm', 68, 73], ['25 μm', 79, 83]]\n",
            "[]\n",
            "[['100%', 38, 41], ['3–4', 48, 50]]\n",
            "[['5–55 Hz', 28, 34]]\n",
            "[['30–130 ms', 34, 42], ['4D', 70, 71]]\n",
            "[['95.81%', 77, 82], ['94.38%', 95, 100], ['SZ.', 106, 108]]\n",
            "[['96.24%', 86, 91], ['93.17%', 104, 109], ['SZ.', 115, 117]]\n",
            "[['(t(39)', 83, 88], ['1.16,', 92, 96], ['0.25)', 102, 106], ['(t(39)', 129, 134], ['1.31,', 138, 142], ['0.20).', 148, 153]]\n",
            "[]\n",
            "[]\n",
            "[['50 μm.', 11, 16]]\n",
            "[['3 months', 28, 35]]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[['36', 62, 63]]\n",
            "[['26', 20, 21]]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[['one', 20, 22]]\n",
            "[['more than 20 passages.', 84, 105]]\n",
            "[['100%', 57, 60]]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[['greater than 70%.', 95, 111]]\n",
            "[]\n",
            "[['25 days.', 99, 106]]\n",
            "[['4 weeks', 105, 111]]\n",
            "[['(6 weeks', 50, 57]]\n",
            "[]\n",
            "[['3', 38, 38]]\n",
            "[]\n",
            "[['200 μm', 12, 17], ['50 μm', 31, 35], ['100 μm', 59, 64]]\n",
            "[['two', 4, 6]]\n",
            "[['8-year', 12, 17]]\n",
            "[['67-year-old', 15, 25]]\n",
            "[['64-year-old', 21, 31]]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[['age-matched', 3, 13], ['64-year-old', 44, 54]]\n",
            "[['3 months', 81, 88]]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[['(A)', 0, 2], ['50 μm);', 55, 61], ['200 μm).', 135, 142]]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[['2 days', 28, 33]]\n",
            "[]\n",
            "[['two', 14, 16]]\n",
            "[['two', 14, 16]]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[['12 weeks', 141, 148]]\n",
            "[]\n",
            "[['48 hr', 39, 43]]\n",
            "[]\n",
            "[]\n",
            "[['80%', 36, 38]]\n",
            "[]\n",
            "[['3 months', 108, 115], ['3B).', 133, 136]]\n",
            "[]\n",
            "[['at 2 months', 61, 71], ['three', 97, 101]]\n",
            "[['3.5–4 months', 41, 52], ['No. 10).', 216, 223]]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[['60%', 47, 49]]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[['10%', 82, 84], ['1X', 106, 107]]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[['10,500×', 36, 42], ['25,000×;', 109, 116], ['2 μm/inset 0.2 μm', 139, 155], ['(J).', 177, 180]]\n",
            "[['5–6 days', 107, 114]]\n",
            "[]\n",
            "[['Ten-day-old', 0, 10]]\n",
            "[['3–4 weeks,', 36, 45], ['Biosciences)', 97, 108], ['(0.1 mg/ml)', 146, 156], ['(10 μg/m).', 170, 179]]\n",
            "[['after 2-4 weeks', 18, 32]]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[['approximately 78 ± 0.6%', 44, 66], ['approximately 88 ± 1.1%', 107, 129], ['approximately 29 ± 0.9%', 169, 191], ['(Fig. 3D).', 227, 236]]\n",
            "[['approximately 20 μg/ml', 83, 104]]\n",
            "[]\n",
            "[]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpK5Bmr7PYLv"
      },
      "source": [
        "Relation Extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5okh0jtzPebT"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJD1YZB3PrNS"
      },
      "source": [
        "def tokenize_and_preserve_labels(sentence, text_labels, tokenizer):\r\n",
        "\r\n",
        "    \"\"\"\r\n",
        "    Word piece tokenization makes it difficult to match word labels\r\n",
        "    back up with individual word pieces. This function tokenizes each\r\n",
        "    word one at a time so that it is easier to preserve the correct\r\n",
        "    label for each subword. It is, of course, a bit slower in processing\r\n",
        "    time, but it will help our model achieve higher accuracy.\r\n",
        "    \"\"\"\r\n",
        "    # print(len(sentence))\r\n",
        "    # print(len(text_labels))\r\n",
        "    tokenized_sentence = []\r\n",
        "    labels = []\r\n",
        "\r\n",
        "    for word, label in zip(sentence, text_labels):\r\n",
        "\r\n",
        "        # Tokenize the word and count # of subwords the word is broken into\r\n",
        "        tokenized_word = tokenizer.tokenize(word)\r\n",
        "        n_subwords = len(tokenized_word)\r\n",
        "\r\n",
        "        # Add the tokenized word to the final tokenized word list\r\n",
        "        tokenized_sentence.extend(tokenized_word)\r\n",
        "\r\n",
        "        # Add the same label to the new list of labels `n_subwords` times\r\n",
        "        labels.extend([label] * n_subwords)\r\n",
        "    # print(len(tokenized_sentence))\r\n",
        "    # print(len(labels))\r\n",
        "    return tokenized_sentence, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PQTlX1HKGe2"
      },
      "source": [
        "lis_en = []\r\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\r\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\r\n",
        "for i in range(len(test_text)):\r\n",
        "  tag={}\r\n",
        "  embed={}\r\n",
        "\r\n",
        "    \r\n",
        "  for j in range(len(Quantity[i])):\r\n",
        "    L=test_text[i].lower()\r\n",
        "    if L.find(Quantity[i][j]):\r\n",
        "      start=[m.start() for m in re.finditer(Quantity[i][j],L)]\r\n",
        "      if(len(start)>0):\r\n",
        "        start_q = start[0]\r\n",
        "        end_q = len(Quantity[i][j]) + start_q - 1\r\n",
        "        tag[\"Q-\"+str(j+1)]=[start_q,end_q,Quantity[i][j]]\r\n",
        "        embed[\"Q-\"+str(j+1)]=np.zeros(768)\r\n",
        "  \r\n",
        "  for j in range(len(Property[i])):\r\n",
        "    test_text[i]=test_text[i].lower()\r\n",
        "    if test_text[i].find(Property[i][j]):\r\n",
        "      \r\n",
        "      if(Property[i][j] in (\"!\", \",\" ,\"\\'\" ,\";\" ,\"\\\"\", \".\", \"-\" ,\"?\",\")\",\"+\",\"-\",\"*\",\"/\",\"%\")):\r\n",
        "        continue\r\n",
        "      start=[m.start() for m in re.finditer(Property[i][j],test_text[i])]\r\n",
        "      \r\n",
        "      if(len(start)>0):\r\n",
        "        start_q = start[0]\r\n",
        "        end_q = len(Property[i][j]) + start_q - 1\r\n",
        "        tag[\"P-\"+str(j+1)]=[start_q,end_q,Property[i][j]]\r\n",
        "        embed[\"P-\"+str(j+1)]=np.zeros(768)\r\n",
        "\r\n",
        "  for j in range(len(Entity[i])):\r\n",
        "    \r\n",
        "    if test_text[i].find(Entity[i][j]):\r\n",
        "      if(Entity[i][j] in (\"!\", \",\" ,\"\\'\" ,\";\" ,'(',\"\\\"\", \".\", \"-\" ,\"?\",\")\",\"+\",\"-\",\"*\",\"/\",\"%\")):\r\n",
        "        continue\r\n",
        "      \r\n",
        "      start=[m.start() for m in re.finditer(Entity[i][j],test_text[i])]\r\n",
        "      # print(start,test_text[i],Entity[i][j])\r\n",
        "      if(len(start)>0):\r\n",
        "        start_q = start[0]\r\n",
        "        end_q = len(Entity[i][j]) + start_q - 1\r\n",
        "        tag[\"E-\"+str(j+1)]=[start_q,end_q,Entity[i][j]]\r\n",
        "        embed[\"E-\"+str(j+1)]=np.zeros(768)\r\n",
        "\r\n",
        "  \r\n",
        "  \r\n",
        "  j=test_text[i]\r\n",
        "  ret = [(m.group(0), m.start(), m.end() - 1) for m in re.finditer(r'\\S+', j)]\r\n",
        "  entities=[]\r\n",
        "  for word in ret:\r\n",
        "    temp=[]\r\n",
        "    for key,ent_doc in tag.items():\r\n",
        "      if(word[1]>=ent_doc[0] and word[2]<=ent_doc[1]):\r\n",
        "        temp.append(key)\r\n",
        "    entities.append(temp)\r\n",
        "  \r\n",
        "\r\n",
        "  sen,label=tokenize_and_preserve_labels(j.split(' '),entities,tokenizer)\r\n",
        "  label.insert(0,[])\r\n",
        "  label.append([])\r\n",
        "  inputs = tokenizer(j, return_tensors=\"pt\")\r\n",
        "  outputs = model(**inputs)\r\n",
        "  \r\n",
        "  last_hidden_states = outputs[0]\r\n",
        "\r\n",
        "  for i in range(len(label)):\r\n",
        "    if(len(label[i])==0):\r\n",
        "      continue\r\n",
        "    else:\r\n",
        "      for k in range(len(label[i])):\r\n",
        "        embed[label[i][k]]=embed[label[i][k]]+last_hidden_states[0][i].detach().numpy()\r\n",
        "  \r\n",
        "  data=[]\r\n",
        "\r\n",
        "  for key,val in tag.items():\r\n",
        "    if(key[0]=='Q'):\r\n",
        "      data.append([\"Quantity\",embed[key],val[2]])\r\n",
        "    elif(key[0]=='P'):\r\n",
        "      data.append([\"MeasuredProperty\",embed[key],val[2]])\r\n",
        "    else:\r\n",
        "      data.append([\"MeasuredEntity\",embed[key],val[2]])    \r\n",
        "  lis_en.append(data)\r\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUuDc0WAQpy1"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "from tensorflow import keras\r\n",
        "from tensorflow.keras import layers\r\n",
        "import numpy as np\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHDaAedSQv16"
      },
      "source": [
        "inputA = layers.Input(shape=(768,))\r\n",
        "inputB = layers.Input(shape=(768,))\r\n",
        "inputC = layers.Input(shape=(1,))\r\n",
        "inputD = layers.Input(shape = (1,))\r\n",
        "\r\n",
        "x = layers.Dense(128, activation=\"relu\")(inputA)\r\n",
        "x = layers.Dense(64, activation=\"relu\")(x)\r\n",
        "\r\n",
        "y = layers.Dense(128, activation=\"relu\")(inputB)\r\n",
        "y = layers.Dense(64, activation=\"relu\")(y)\r\n",
        "\r\n",
        "c = layers.Dense(1, activation=\"relu\")(inputC)\r\n",
        "\r\n",
        "d = layers.Dense(1, activation=\"relu\")(inputD)\r\n",
        "\r\n",
        "\r\n",
        "com = layers.concatenate([x, y, c, d])\r\n",
        "z = layers.Dense(32, activation=\"relu\")(com)\r\n",
        "z = layers.Dense(4,  activation='softmax')(z)\r\n",
        "model = keras.Model(inputs=[inputA, inputB, inputC, inputD], outputs=z)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPD1n7G7QxE-"
      },
      "source": [
        "weights = [15.593220338983052, 10.415094339622641, 33.65853658536585, 2.323232323232323]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSLnwk6qQz71"
      },
      "source": [
        "from keras import backend as K\r\n",
        "def weighted_categorical_crossentropy(weights):\r\n",
        "\r\n",
        "    weights = K.variable(weights)\r\n",
        "        \r\n",
        "    def loss(y_true, y_pred):\r\n",
        "\r\n",
        "        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\r\n",
        "\r\n",
        "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\r\n",
        "\r\n",
        "        loss = y_true * K.log(y_pred) * weights\r\n",
        "        loss = -K.sum(loss, -1)\r\n",
        "        return loss\r\n",
        "    \r\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "waFJwVNMQ3Kv"
      },
      "source": [
        "model.compile(optimizer='adam',\r\n",
        "              loss= weighted_categorical_crossentropy(weights),\r\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8rlNg3IQ6HZ"
      },
      "source": [
        "from keras.models import load_model\r\n",
        "model.load_weights('/content/drive/My Drive/Model/rel_wt_new.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nb2NpDFFPlNq"
      },
      "source": [
        "idx2tok = {}\r\n",
        "idx2tok[\"Quantity\"] = 1\r\n",
        "idx2tok[\"MeasuredEntity\"] = 2\r\n",
        "idx2tok[\"MeasuredProperty\"] = 3\r\n",
        "idx2tok[\"Qualifier\"] = 4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnKIRh1mQZZM"
      },
      "source": [
        "tok2idx = {}\r\n",
        "tok2idx[0] = \"HasProperty\"\r\n",
        "tok2idx[1] = \"HasQuantity\"\r\n",
        "tok2idx[2] = \"Qualifies\"\r\n",
        "tok2idx[3] = \"norelation\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3lpRs3FQcLR"
      },
      "source": [
        "relation = []\r\n",
        "for sen in lis_en:\r\n",
        "  ans = []\r\n",
        "  for i in range(len(sen)):\r\n",
        "    for j in range(i+1, len(sen)):\r\n",
        "      x1 = sen[i][1].reshape([1,768])\r\n",
        "      x2 = sen[j][1].reshape([1,768])\r\n",
        "      x3 = np.array([idx2tok[sen[i][0]]]).reshape([1,1])\r\n",
        "      x4 = np.array([idx2tok[sen[j][0]]]).reshape([1,1])\r\n",
        "      y_pred = model.predict([x1,x2,x3,x4])\r\n",
        "      rel_pred = np.argmax(y_pred)\r\n",
        "      ans.append([sen[i][2], sen[j][2], tok2idx[rel_pred]])\r\n",
        "  \r\n",
        "  relation.append(ans)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yA_KZdBSRF3c",
        "outputId": "62ebd7b6-e7a9-43f7-a22a-e9cb1c43b53a"
      },
      "source": [
        "relation"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [['10 μm.', 'scale bars', 'HasQuantity'],\n",
              "  ['10 μm.', 'scale bars', 'HasQuantity'],\n",
              "  ['scale bars', 'scale bars', 'HasProperty']],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [['96', 'regions', 'HasQuantity']],\n",
              " [['5 mm per side', 'grid with voxels', 'HasQuantity'],\n",
              "  ['5 mm per side', 'regional masks', 'HasQuantity'],\n",
              "  ['5 mm per side', 'source grid', 'HasQuantity'],\n",
              "  ['5 mm per side', 'voxels', 'HasQuantity'],\n",
              "  ['5 mm per side', 'side', 'HasQuantity'],\n",
              "  ['grid with voxels', 'regional masks', 'HasProperty'],\n",
              "  ['grid with voxels', 'source grid', 'HasProperty'],\n",
              "  ['grid with voxels', 'voxels', 'Qualifies'],\n",
              "  ['grid with voxels', 'side', 'HasQuantity'],\n",
              "  ['regional masks', 'source grid', 'HasProperty'],\n",
              "  ['regional masks', 'voxels', 'HasProperty'],\n",
              "  ['regional masks', 'side', 'HasProperty'],\n",
              "  ['source grid', 'voxels', 'HasProperty'],\n",
              "  ['source grid', 'side', 'HasProperty'],\n",
              "  ['voxels', 'side', 'HasProperty']],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [['regions', 'interest', 'Qualifies'],\n",
              "  ['regions', 'ROIs', 'HasProperty'],\n",
              "  ['interest', 'ROIs', 'HasProperty']],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [['two', 'after 6 months', 'norelation'],\n",
              "  ['two', 'used', 'HasQuantity'],\n",
              "  ['two', 'withdrawn', 'HasQuantity'],\n",
              "  ['two', 'double-blind clinical trials', 'HasQuantity'],\n",
              "  ['two', 'immunosuppressive', 'HasQuantity'],\n",
              "  ['after 6 months', 'used', 'HasProperty'],\n",
              "  ['after 6 months', 'withdrawn', 'HasProperty'],\n",
              "  ['after 6 months', 'double-blind clinical trials', 'HasProperty'],\n",
              "  ['after 6 months', 'immunosuppressive', 'HasProperty'],\n",
              "  ['used', 'withdrawn', 'Qualifies'],\n",
              "  ['used', 'double-blind clinical trials', 'norelation'],\n",
              "  ['used', 'immunosuppressive', 'Qualifies'],\n",
              "  ['withdrawn', 'double-blind clinical trials', 'HasQuantity'],\n",
              "  ['withdrawn', 'immunosuppressive', 'Qualifies'],\n",
              "  ['double-blind clinical trials', 'immunosuppressive', 'HasProperty']],\n",
              " [['more than 50,000', 'survived', 'HasQuantity']],\n",
              " [['1/10 that', 'expression', 'HasQuantity'],\n",
              "  ['1/10 that', 'increased', 'HasQuantity'],\n",
              "  ['1/10 that', 'response', 'HasQuantity'],\n",
              "  ['1/10 that', 'expression level', 'HasQuantity'],\n",
              "  ['1/10 that', 'untreated monkey peripheral blood cells', 'HasProperty'],\n",
              "  ['expression', 'increased', 'HasProperty'],\n",
              "  ['expression', 'response', 'HasProperty'],\n",
              "  ['expression', 'expression level', 'HasProperty'],\n",
              "  ['expression', 'untreated monkey peripheral blood cells', 'HasProperty'],\n",
              "  ['increased', 'response', 'Qualifies'],\n",
              "  ['increased', 'expression level', 'HasProperty'],\n",
              "  ['increased', 'untreated monkey peripheral blood cells', 'HasProperty'],\n",
              "  ['response', 'expression level', 'HasProperty'],\n",
              "  ['response', 'untreated monkey peripheral blood cells', 'HasProperty'],\n",
              "  ['expression level',\n",
              "   'untreated monkey peripheral blood cells',\n",
              "   'HasProperty']],\n",
              " [['2 months,', '3.5–4 months', 'HasQuantity']],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [['1/100', 'expression', 'HasQuantity'],\n",
              "  ['1/100', 'peripheral blood cells', 'HasQuantity'],\n",
              "  ['expression', 'peripheral blood cells', 'HasProperty']],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [['6', 'em', 'HasProperty']],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [['125 μm', '25 μm', 'HasQuantity'],\n",
              "  ['125 μm', 'Fetal hRPE', 'HasQuantity'],\n",
              "  ['25 μm', 'Fetal hRPE', 'HasProperty']],\n",
              " [],\n",
              " [['100%', '3–4', 'HasQuantity'],\n",
              "  ['100%', '20', 'HasQuantity'],\n",
              "  ['100%', 'control line 20', 'HasQuantity'],\n",
              "  ['100%', 'cultures', 'HasQuantity'],\n",
              "  ['3–4', '20', 'HasQuantity'],\n",
              "  ['3–4', 'control line 20', 'HasQuantity'],\n",
              "  ['3–4', 'cultures', 'HasQuantity'],\n",
              "  ['20', 'control line 20', 'HasQuantity'],\n",
              "  ['20', 'cultures', 'HasQuantity'],\n",
              "  ['control line 20', 'cultures', 'HasProperty']],\n",
              " [['5–55', 'pass filter', 'HasQuantity']],\n",
              " [['30–130 ms', '4d', 'HasQuantity'],\n",
              "  ['30–130 ms', 'volumes', 'HasQuantity'],\n",
              "  ['4d', 'volumes', 'HasQuantity']],\n",
              " [['95.81%', '94.38%', 'HasQuantity'],\n",
              "  ['95.81%', 'HC', 'HasQuantity'],\n",
              "  ['95.81%', 'SZ', 'HasQuantity'],\n",
              "  ['94.38%', 'HC', 'HasQuantity'],\n",
              "  ['94.38%', 'SZ', 'HasQuantity'],\n",
              "  ['HC', 'SZ', 'HasProperty']],\n",
              " [['96.24%', '93.17%', 'HasQuantity'],\n",
              "  ['96.24%', 'percent variance', 'HasQuantity'],\n",
              "  ['93.17%', 'percent variance', 'HasQuantity']],\n",
              " [['1.16,', '0.25', 'HasQuantity'],\n",
              "  ['1.16,', '1.31,', 'HasQuantity'],\n",
              "  ['1.16,', 'no group differences', 'HasQuantity'],\n",
              "  ['1.16,', 'percent variance explained', 'HasQuantity'],\n",
              "  ['1.16,', 'grad', 'HasQuantity'],\n",
              "  ['1.16,', '39', 'HasQuantity'],\n",
              "  ['1.16,', 'magnetometer data', 'HasQuantity'],\n",
              "  ['0.25', '1.31,', 'HasQuantity'],\n",
              "  ['0.25', 'no group differences', 'HasQuantity'],\n",
              "  ['0.25', 'percent variance explained', 'HasProperty'],\n",
              "  ['0.25', 'grad', 'HasProperty'],\n",
              "  ['0.25', '39', 'HasProperty'],\n",
              "  ['0.25', 'magnetometer data', 'HasProperty'],\n",
              "  ['1.31,', 'no group differences', 'HasQuantity'],\n",
              "  ['1.31,', 'percent variance explained', 'HasQuantity'],\n",
              "  ['1.31,', 'grad', 'HasQuantity'],\n",
              "  ['1.31,', '39', 'HasQuantity'],\n",
              "  ['1.31,', 'magnetometer data', 'HasQuantity'],\n",
              "  ['no group differences', 'percent variance explained', 'norelation'],\n",
              "  ['no group differences', 'grad', 'Qualifies'],\n",
              "  ['no group differences', '39', 'Qualifies'],\n",
              "  ['no group differences', 'magnetometer data', 'HasProperty'],\n",
              "  ['percent variance explained', 'grad', 'Qualifies'],\n",
              "  ['percent variance explained', '39', 'Qualifies'],\n",
              "  ['percent variance explained', 'magnetometer data', 'HasProperty'],\n",
              "  ['grad', '39', 'Qualifies'],\n",
              "  ['grad', 'magnetometer data', 'HasProperty'],\n",
              "  ['39', 'magnetometer data', 'HasProperty']],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [['36', '36 clones', 'HasQuantity']],\n",
              " [['26', 's', 'HasQuantity'],\n",
              "  ['26', 'found', 'HasQuantity'],\n",
              "  ['26', 'cassette', 'HasQuantity'],\n",
              "  ['26', 'clones', 'HasQuantity'],\n",
              "  ['s', 'found', 'Qualifies'],\n",
              "  ['s', 'cassette', 'Qualifies'],\n",
              "  ['s', 'clones', 'Qualifies'],\n",
              "  ['found', 'cassette', 'HasQuantity'],\n",
              "  ['found', 'clones', 'Qualifies'],\n",
              "  ['cassette', 'clones', 'Qualifies']],\n",
              " [],\n",
              " [['these clones, clone 23', 'SO', 'HasProperty']],\n",
              " [],\n",
              " [],\n",
              " [['SO', 'cells', 'HasProperty']],\n",
              " [],\n",
              " [['one', 'of SOX2, hSOX2', 'HasQuantity'],\n",
              "  ['one', 'hSOX2-25 and wild-type', 'HasQuantity'],\n",
              "  ['of SOX2, hSOX2', 'hSOX2-25 and wild-type', 'HasProperty']],\n",
              " [['more than 20 passages.', 'constant', 'HasQuantity']],\n",
              " [['100%', 'unofluores', 'HasQuantity']],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [['greater than 70%.', 'gene targeting rates', 'HasQuantity']],\n",
              " [],\n",
              " [['25 days.', 'immunocytochemistry', 'HasQuantity'],\n",
              "  ['25 days.', 'cells differentiated', 'HasQuantity'],\n",
              "  ['immunocytochemistry', 'cells differentiated', 'HasProperty']],\n",
              " [['4 weeks', 'survived', 'HasQuantity']],\n",
              " [],\n",
              " [],\n",
              " [['3', '3 independent experiments', 'HasQuantity']],\n",
              " [],\n",
              " [['200 μm', '50 μm', 'HasQuantity'],\n",
              "  ['200 μm', '100 μm', 'HasQuantity'],\n",
              "  ['200 μm', 'bars', 'HasQuantity'],\n",
              "  ['200 μm', 'insets of', 'HasQuantity'],\n",
              "  ['50 μm', '100 μm', 'HasQuantity'],\n",
              "  ['50 μm', 'bars', 'HasQuantity'],\n",
              "  ['50 μm', 'insets of', 'HasQuantity'],\n",
              "  ['100 μm', 'bars', 'HasQuantity'],\n",
              "  ['100 μm', 'insets of', 'HasQuantity'],\n",
              "  ['bars', 'insets of', 'HasQuantity']],\n",
              " [['two', 'FTD patients', 'HasQuantity']],\n",
              " [],\n",
              " [['67-year-', 'tested negative', 'HasProperty'],\n",
              "  ['67-year-', 'patient,', 'HasProperty'],\n",
              "  ['tested negative', 'patient,', 'HasQuantity']],\n",
              " [['64-year-', 'patient,', 'HasProperty']],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [['64-year-', 'subject,', 'HasProperty']],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [['50 μm', '200 μm', 'HasQuantity'],\n",
              "  ['50 μm', 'stained for pan-cyt', 'HasQuantity'],\n",
              "  ['200 μm', 'stained for pan-cyt', 'HasQuantity']],\n",
              " [['ne', 'clusters', 'Qualifies'],\n",
              "  ['ne', 'eminal anlage', 'HasProperty'],\n",
              "  ['clusters', 'eminal anlage', 'HasProperty']],\n",
              " [],\n",
              " [['transplant', 'G', 'Qualifies']],\n",
              " [],\n",
              " [],\n",
              " [['2 days', 'fiber outgro', 'HasQuantity']],\n",
              " [],\n",
              " [['two', 'two animals', 'norelation']],\n",
              " [],\n",
              " [['two', 'two animals', 'norelation']],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [['12 weeks', 'deficiency', 'HasQuantity']],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [['80%', 'resulting DE population', 'HasQuantity']],\n",
              " [],\n",
              " [['up', 'allograft', 'Qualifies']],\n",
              " [],\n",
              " [],\n",
              " [['at 2 months', 'three', 'HasQuantity'],\n",
              "  ['at 2 months', 'increased', 'HasQuantity'],\n",
              "  ['at 2 months', 'animals', 'HasQuantity'],\n",
              "  ['three', 'increased', 'HasQuantity'],\n",
              "  ['three', 'animals', 'HasQuantity'],\n",
              "  ['increased', 'animals', 'Qualifies']],\n",
              " [['3.5–4 months', 'up', 'HasQuantity']],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [['10%', '1x', 'HasQuantity'],\n",
              "  ['10%', 'ssential amino acids, and penicillin/stre', 'HasQuantity'],\n",
              "  ['10%', 'cells', 'HasQuantity'],\n",
              "  ['10%', 'etal bovine serum', 'HasQuantity'],\n",
              "  ['10%', 'nonessential amino acids,', 'HasQuantity'],\n",
              "  ['10%', 'ici', 'HasQuantity'],\n",
              "  ['1x', 'ssential amino acids, and penicillin/stre', 'HasQuantity'],\n",
              "  ['1x', 'cells', 'HasQuantity'],\n",
              "  ['1x', 'etal bovine serum', 'HasQuantity'],\n",
              "  ['1x', 'nonessential amino acids,', 'HasQuantity'],\n",
              "  ['1x', 'ici', 'HasQuantity'],\n",
              "  ['ssential amino acids, and penicillin/stre', 'cells', 'HasProperty'],\n",
              "  ['ssential amino acids, and penicillin/stre',\n",
              "   'etal bovine serum',\n",
              "   'HasProperty'],\n",
              "  ['ssential amino acids, and penicillin/stre',\n",
              "   'nonessential amino acids,',\n",
              "   'HasProperty'],\n",
              "  ['ssential amino acids, and penicillin/stre', 'ici', 'HasProperty'],\n",
              "  ['cells', 'etal bovine serum', 'HasProperty'],\n",
              "  ['cells', 'nonessential amino acids,', 'HasProperty'],\n",
              "  ['cells', 'ici', 'HasProperty'],\n",
              "  ['etal bovine serum', 'nonessential amino acids,', 'HasProperty'],\n",
              "  ['etal bovine serum', 'ici', 'norelation'],\n",
              "  ['nonessential amino acids,', 'ici', 'HasProperty']],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [['10,500×', '25,000×', 'HasQuantity'],\n",
              "  ['10,500×', '2 μm/', 'norelation'],\n",
              "  ['10,500×', '0.2 μm', 'norelation'],\n",
              "  ['10,500×', 'magnification', 'HasQuantity'],\n",
              "  ['10,500×', 'distance', 'HasQuantity'],\n",
              "  ['10,500×', 'right micrographs', 'norelation'],\n",
              "  ['10,500×', '/inset', 'HasQuantity'],\n",
              "  ['25,000×', '2 μm/', 'norelation'],\n",
              "  ['25,000×', '0.2 μm', 'norelation'],\n",
              "  ['25,000×', 'magnification', 'HasProperty'],\n",
              "  ['25,000×', 'distance', 'HasProperty'],\n",
              "  ['25,000×', 'right micrographs', 'HasQuantity'],\n",
              "  ['25,000×', '/inset', 'HasProperty'],\n",
              "  ['2 μm/', '0.2 μm', 'HasQuantity'],\n",
              "  ['2 μm/', 'magnification', 'HasQuantity'],\n",
              "  ['2 μm/', 'distance', 'HasQuantity'],\n",
              "  ['2 μm/', 'right micrographs', 'HasQuantity'],\n",
              "  ['2 μm/', '/inset', 'HasQuantity'],\n",
              "  ['0.2 μm', 'magnification', 'HasQuantity'],\n",
              "  ['0.2 μm', 'distance', 'HasQuantity'],\n",
              "  ['0.2 μm', 'right micrographs', 'HasQuantity'],\n",
              "  ['0.2 μm', '/inset', 'HasQuantity'],\n",
              "  ['magnification', 'distance', 'Qualifies'],\n",
              "  ['magnification', 'right micrographs', 'Qualifies'],\n",
              "  ['magnification', '/inset', 'Qualifies'],\n",
              "  ['distance', 'right micrographs', 'HasQuantity'],\n",
              "  ['distance', '/inset', 'HasQuantity'],\n",
              "  ['right micrographs', '/inset', 'norelation']],\n",
              " [['5–6 days', 'suspension', 'HasQuantity']],\n",
              " [],\n",
              " [],\n",
              " [['3–4 weeks,', '0.1 mg/ml', 'HasQuantity'],\n",
              "  ['3–4 weeks,', '10 μg', 'HasQuantity'],\n",
              "  ['3–4 weeks,', 'di', 'HasQuantity'],\n",
              "  ['3–4 weeks,', 'cia', 'HasQuantity'],\n",
              "  ['0.1 mg/ml', '10 μg', 'HasProperty'],\n",
              "  ['0.1 mg/ml', 'di', 'HasProperty'],\n",
              "  ['0.1 mg/ml', 'cia', 'HasProperty'],\n",
              "  ['10 μg', 'di', 'HasProperty'],\n",
              "  ['10 μg', 'cia', 'HasProperty'],\n",
              "  ['di', 'cia', 'Qualifies']],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [['approximately 78 ± 0.6%', 'approximately 88 ± 1.1%', 'Qualifies'],\n",
              "  ['approximately 78 ± 0.6%', 'approximately 29 ± 0.9%', 'Qualifies'],\n",
              "  ['approximately 78 ± 0.6%', 'uantitative imaging analysis', 'HasQuantity'],\n",
              "  ['approximately 78 ± 0.6%', 'cells', 'HasQuantity'],\n",
              "  ['approximately 78 ± 0.6%', '17', 'HasQuantity'],\n",
              "  ['approximately 78 ± 0.6%', 'cells', 'HasQuantity'],\n",
              "  ['approximately 78 ± 0.6%', 'cells', 'HasQuantity'],\n",
              "  ['approximately 88 ± 1.1%', 'approximately 29 ± 0.9%', 'Qualifies'],\n",
              "  ['approximately 88 ± 1.1%', 'uantitative imaging analysis', 'HasQuantity'],\n",
              "  ['approximately 88 ± 1.1%', 'cells', 'HasQuantity'],\n",
              "  ['approximately 88 ± 1.1%', '17', 'HasQuantity'],\n",
              "  ['approximately 88 ± 1.1%', 'cells', 'HasQuantity'],\n",
              "  ['approximately 88 ± 1.1%', 'cells', 'HasQuantity'],\n",
              "  ['approximately 29 ± 0.9%', 'uantitative imaging analysis', 'HasQuantity'],\n",
              "  ['approximately 29 ± 0.9%', 'cells', 'HasQuantity'],\n",
              "  ['approximately 29 ± 0.9%', '17', 'HasQuantity'],\n",
              "  ['approximately 29 ± 0.9%', 'cells', 'HasQuantity'],\n",
              "  ['approximately 29 ± 0.9%', 'cells', 'HasQuantity'],\n",
              "  ['uantitative imaging analysis', 'cells', 'HasProperty'],\n",
              "  ['uantitative imaging analysis', '17', 'HasProperty'],\n",
              "  ['uantitative imaging analysis', 'cells', 'HasProperty'],\n",
              "  ['uantitative imaging analysis', 'cells', 'HasProperty'],\n",
              "  ['cells', '17', 'HasProperty'],\n",
              "  ['cells', 'cells', 'HasProperty'],\n",
              "  ['cells', 'cells', 'HasProperty'],\n",
              "  ['17', 'cells', 'HasProperty'],\n",
              "  ['17', 'cells', 'HasProperty'],\n",
              "  ['cells', 'cells', 'HasProperty']],\n",
              " [],\n",
              " [['approximately 20 μg/ml', 'media', 'HasQuantity']],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " []]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXjSrT69WeOe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}